
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept &#8212; Instrumentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
const load = async () => {
    await mermaid.run();
    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_to_add_zoom = 1 === -1 ? all_mermaids.length : 1;
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");
    if(mermaids_to_add_zoom > 0) {
        var svgs = d3.selectAll(".mermaid[data-zoom-id=id-b4150d35-2b9e-4046-8c9a-d86234cb7c66] svg");
        if(all_mermaids.length !== mermaids_processed.length) {
            // try again in a sec, wait for mermaids to load
            setTimeout(load, 200);
            return;
        } else if(svgs.size() !== mermaids_to_add_zoom) {
            // try again in a sec, wait for mermaids to load
            setTimeout(load, 200);
            return;
        } else {
            svgs.each(function() {
                var svg = d3.select(this);
                svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                var inner = svg.select("g");
                var zoom = d3.zoom().on("zoom", function(event) {
                    inner.attr("transform", event.transform);
                });
                svg.call(zoom);
            });
        }
    }
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'concept';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Flow" href="flow.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Instrumentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="flow.html">Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="propagator_types.html">Understanding Propagator Types in Distributed Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="temp.html">Documentation: cryostorm Instrumentation Library</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/gao-hongnan/instrumentation/issues/new?title=Issue%20on%20page%20%2Fconcept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button"
   title="Open an issue"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-understanding-complex-systems">The Problem: Understanding Complex Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-observability-with-opentelemetry">The Solution: Observability with OpenTelemetry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#managing-telemetry-data-with-the-otel-collector">Managing Telemetry Data with the OTel Collector</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-networking-concepts-for-services-and-containers">Basic Networking Concepts for Services and Containers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#networking-101">Networking 101</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#networking-in-containers-docker-docker-compose">Networking in Containers (Docker / Docker Compose)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-telemetry-flow">Anatomy of Telemetry Flow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generation-phase-application-level-telemetry-instantiation">1. Generation Phase: Application-Level Telemetry Instantiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transmission-phase-otlp-network-protocol">2. Transmission Phase: OTLP Network Protocol</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reception-phase-collector-data-ingestion">3. Reception Phase: Collector Data Ingestion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-phase-collector-pipeline-execution">4. Processing Phase: Collector Pipeline Execution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exportation-phase-data-egress-to-backends">5. Exportation Phase: Data Egress to Backends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storage-analysis-phase-backend-systems">6. Storage &amp; Analysis Phase: Backend Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#systemic-considerations">7. Systemic Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-telemetry-flow-deep-research-from-openai-we-keep-it">Anatomy of Telemetry Flow (Deep Research From OpenAI, We Keep It)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instrumentation-in-the-application-opentelemetry-sdk">Instrumentation in the Application (OpenTelemetry SDK)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#telemetry-export-via-otlp-opentelemetry-protocol">Telemetry Export via OTLP (OpenTelemetry Protocol)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-opentelemetry-collectors-role-and-components">The OpenTelemetry Collector’s Role and Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receivers-otlp-receiver-grpc-http">Receivers – OTLP Receiver (gRPC/HTTP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processors-batch-memory-limiter-resource">Processors – Batch, Memory Limiter, Resource</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exporters-otlp-to-jaeger-prometheus-debug">Exporters – OTLP (to Jaeger), Prometheus, Debug</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-health-check-pprof-zpages">Extensions – Health Check, Pprof, ZPages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-collector-central-telemetry-processor">The Collector: Central Telemetry Processor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-note-on-exporters-understand-this-first">Important Note on Exporters (Understand this first)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receivers-receiving-data"><code class="docutils literal notranslate"><span class="pre">receivers</span></code> - Receiving Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processors-modifying-data-in-flight"><code class="docutils literal notranslate"><span class="pre">processors</span></code> - Modifying Data In-Flight</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-configs-in-sdk-app-level-vs-collector-config">Difference between configs in SDK/app level vs collector config</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exporters-sending-data-out"><code class="docutils literal notranslate"><span class="pre">exporters</span></code> - Sending Data Out</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#docker-dns-resolution-and-why-hostname-is-jaeger">Docker DNS Resolution And Why Hostname is Jaeger?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-exporter">Debug Exporter</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-exporter-in-collector-config">Debug Exporter in Collector Config</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#console-logging-in-application-code">Console Logging in Application Code</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-each">When to Use Each</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-adding-auxiliary-capabilities"><code class="docutils literal notranslate"><span class="pre">extensions</span></code> - Adding Auxiliary Capabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#service-tying-everything-together"><code class="docutils literal notranslate"><span class="pre">service</span></code> - Tying Everything Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#motivation" id="id1">Motivation</a></p>
<ul>
<li><p><a class="reference internal" href="#the-problem-understanding-complex-systems" id="id2">The Problem: Understanding Complex Systems</a></p></li>
<li><p><a class="reference internal" href="#the-solution-observability-with-opentelemetry" id="id3">The Solution: Observability with OpenTelemetry</a></p></li>
<li><p><a class="reference internal" href="#managing-telemetry-data-with-the-otel-collector" id="id4">Managing Telemetry Data with the OTel Collector</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#basic-networking-concepts-for-services-and-containers" id="id5">Basic Networking Concepts for Services and Containers</a></p>
<ul>
<li><p><a class="reference internal" href="#networking-101" id="id6">Networking 101</a></p></li>
<li><p><a class="reference internal" href="#networking-in-containers-docker-docker-compose" id="id7">Networking in Containers (Docker / Docker Compose)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#anatomy-of-telemetry-flow" id="id8">Anatomy of Telemetry Flow</a></p>
<ul>
<li><p><a class="reference internal" href="#generation-phase-application-level-telemetry-instantiation" id="id9">1. Generation Phase: Application-Level Telemetry Instantiation</a></p></li>
<li><p><a class="reference internal" href="#transmission-phase-otlp-network-protocol" id="id10">2. Transmission Phase: OTLP Network Protocol</a></p></li>
<li><p><a class="reference internal" href="#reception-phase-collector-data-ingestion" id="id11">3. Reception Phase: Collector Data Ingestion</a></p></li>
<li><p><a class="reference internal" href="#processing-phase-collector-pipeline-execution" id="id12">4. Processing Phase: Collector Pipeline Execution</a></p></li>
<li><p><a class="reference internal" href="#exportation-phase-data-egress-to-backends" id="id13">5. Exportation Phase: Data Egress to Backends</a></p></li>
<li><p><a class="reference internal" href="#storage-analysis-phase-backend-systems" id="id14">6. Storage &amp; Analysis Phase: Backend Systems</a></p></li>
<li><p><a class="reference internal" href="#systemic-considerations" id="id15">7. Systemic Considerations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#anatomy-of-telemetry-flow-deep-research-from-openai-we-keep-it" id="id16">Anatomy of Telemetry Flow (Deep Research From OpenAI, We Keep It)</a></p>
<ul>
<li><p><a class="reference internal" href="#instrumentation-in-the-application-opentelemetry-sdk" id="id17">Instrumentation in the Application (OpenTelemetry SDK)</a></p></li>
<li><p><a class="reference internal" href="#telemetry-export-via-otlp-opentelemetry-protocol" id="id18">Telemetry Export via OTLP (OpenTelemetry Protocol)</a></p></li>
<li><p><a class="reference internal" href="#the-opentelemetry-collectors-role-and-components" id="id19">The OpenTelemetry Collector’s Role and Components</a></p></li>
<li><p><a class="reference internal" href="#receivers-otlp-receiver-grpc-http" id="id20">Receivers – OTLP Receiver (gRPC/HTTP)</a></p></li>
<li><p><a class="reference internal" href="#processors-batch-memory-limiter-resource" id="id21">Processors – Batch, Memory Limiter, Resource</a></p></li>
<li><p><a class="reference internal" href="#exporters-otlp-to-jaeger-prometheus-debug" id="id22">Exporters – OTLP (to Jaeger), Prometheus, Debug</a></p></li>
<li><p><a class="reference internal" href="#extensions-health-check-pprof-zpages" id="id23">Extensions – Health Check, Pprof, ZPages</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-collector-central-telemetry-processor" id="id24">The Collector: Central Telemetry Processor</a></p>
<ul>
<li><p><a class="reference internal" href="#important-note-on-exporters-understand-this-first" id="id25">Important Note on Exporters (Understand this first)</a></p></li>
<li><p><a class="reference internal" href="#receivers-receiving-data" id="id26"><code class="docutils literal notranslate"><span class="pre">receivers</span></code> - Receiving Data</a></p></li>
<li><p><a class="reference internal" href="#processors-modifying-data-in-flight" id="id27"><code class="docutils literal notranslate"><span class="pre">processors</span></code> - Modifying Data In-Flight</a></p>
<ul>
<li><p><a class="reference internal" href="#difference-between-configs-in-sdk-app-level-vs-collector-config" id="id28">Difference between configs in SDK/app level vs collector config</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#exporters-sending-data-out" id="id29"><code class="docutils literal notranslate"><span class="pre">exporters</span></code> - Sending Data Out</a></p>
<ul>
<li><p><a class="reference internal" href="#docker-dns-resolution-and-why-hostname-is-jaeger" id="id30">Docker DNS Resolution And Why Hostname is Jaeger?</a></p></li>
<li><p><a class="reference internal" href="#debug-exporter" id="id31">Debug Exporter</a></p>
<ul>
<li><p><a class="reference internal" href="#debug-exporter-in-collector-config" id="id32">Debug Exporter in Collector Config</a></p></li>
<li><p><a class="reference internal" href="#console-logging-in-application-code" id="id33">Console Logging in Application Code</a></p></li>
<li><p><a class="reference internal" href="#key-differences" id="id34">Key Differences</a></p></li>
<li><p><a class="reference internal" href="#when-to-use-each" id="id35">When to Use Each</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#extensions-adding-auxiliary-capabilities" id="id36"><code class="docutils literal notranslate"><span class="pre">extensions</span></code> - Adding Auxiliary Capabilities</a></p></li>
<li><p><a class="reference internal" href="#service-tying-everything-together" id="id37"><code class="docutils literal notranslate"><span class="pre">service</span></code> - Tying Everything Together</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id38">Summary</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id39">References</a></p></li>
</ul>
</nav>
<section id="motivation">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<section id="the-problem-understanding-complex-systems">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">The Problem: Understanding Complex Systems</a><a class="headerlink" href="#the-problem-understanding-complex-systems" title="Link to this heading">#</a></h3>
<p>Modern software, especially web services, often consists of many interconnected
parts (microservices, databases, external APIs). When something goes wrong (like
a slow request or an error), figuring out <em>where</em> and <em>why</em> can be incredibly
difficult. It’s like trying to find a single faulty wire in a huge, tangled
mess.</p>
</section>
<section id="the-solution-observability-with-opentelemetry">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">The Solution: Observability with OpenTelemetry</a><a class="headerlink" href="#the-solution-observability-with-opentelemetry" title="Link to this heading">#</a></h3>
<p><strong>Observability</strong> is about designing your systems so you can understand their
internal state just by observing their external outputs. Think of it like a
car’s dashboard – it gives you speed (metric), engine temperature (metric), and
maybe a check engine light (log/event) without you needing to open the hood.</p>
<p>To achieve observability, we collect <strong>telemetry data</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Traces:</strong> Record the path of a single request as it travels through
different services. Helps answer “Where did this request go?” and “Where did
it spend its time?”.</p></li>
<li><p><strong>Metrics:</strong> Numerical measurements over time (e.g., number of requests per
second, CPU usage, error rate). Helps answer “How is the system performing
overall?” and “Are resource limits being hit?”.</p></li>
<li><p><strong>Logs:</strong> Timestamped text records of specific events (e.g., an error
occurred, user logged in, configuration loaded). Helps answer “What specific
event happened at this time?”.</p></li>
</ol>
<p><strong>OpenTelemetry (OTel)</strong> is an open-source industry standard and collection of
tools, APIs (Application Programming Interfaces), and SDKs (Software Development
Kits) for generating, collecting, and exporting this telemetry data. It provides
a vendor-neutral way to instrument your code, meaning you aren’t locked into a
specific monitoring tool provider. Your applications use these OTel libraries to
generate the traces, metrics, and logs described above.</p>
</section>
<section id="managing-telemetry-data-with-the-otel-collector">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Managing Telemetry Data with the OTel Collector</a><a class="headerlink" href="#managing-telemetry-data-with-the-otel-collector" title="Link to this heading">#</a></h3>
<p>Now that your applications are generating this valuable telemetry data, how do
you get it efficiently and reliably to the systems where you’ll analyze and
visualize it? Sending data directly from every application instance to
potentially multiple backend systems (like a logging platform, a metrics
database, <em>and</em> a tracing system) can become complex and inefficient.</p>
<p>This is where the
<a class="reference external" href="https://opentelemetry.io/docs/collector/"><strong>OpenTelemetry Collector</strong></a> becomes
essential. The Collector is a standalone application designed specifically to
receive, process, and export telemetry data. It acts as a flexible and robust
agent or gateway in your observability pipeline. Think of it like a highly
configurable mail sorting facility for your observability signals:</p>
<ul class="simple">
<li><p><strong>Receives:</strong> It listens for incoming telemetry data from various sources,
primarily your instrumented applications using protocols like
<code class="docutils literal notranslate"><span class="pre">OTLP</span> <span class="pre">(OpenTelemetry</span> <span class="pre">Protocol)</span></code>, but potentially other formats as well.</p></li>
<li><p><strong>Processes:</strong> Before forwarding the data, the Collector can perform various
processing tasks. This might include filtering out sensitive information,
adding common attributes (like deployment environment), modifying data
(e.g., renaming metrics), sampling traces to reduce volume, or batching data
for more efficient export.</p></li>
<li><p><strong>Exports:</strong> Finally, it sends the processed data to one or more configured
backend systems. These could be open-source tools like <code class="docutils literal notranslate"><span class="pre">Prometheus</span></code>,
<code class="docutils literal notranslate"><span class="pre">Jaeger</span></code>, or <code class="docutils literal notranslate"><span class="pre">Loki</span></code>, or commercial observability platforms.</p></li>
</ul>
<p>Using a Collector centralizes the logic for handling telemetry data. Your
applications only need to be configured to send data to the Collector’s address.
The Collector then takes on the responsibility of aggregation, transformation,
and routing to the final destinations. This decoupling makes your architecture
more resilient and easier to manage, especially as your systems or backend
choices evolve. The specific configuration defining how the collector receives,
processes, and exports data is managed through a configuration file, such as the
<a class="reference download internal" download="" href="_downloads/bcd84ad6c936b06f226d4b1b1611d809/collector-config.yaml"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">collector-config.yaml</span></code></span></a> file mentioned.</p>
</section>
</section>
<section id="basic-networking-concepts-for-services-and-containers">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Basic Networking Concepts for Services and Containers</a><a class="headerlink" href="#basic-networking-concepts-for-services-and-containers" title="Link to this heading">#</a></h2>
<section id="networking-101">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Networking 101</a><a class="headerlink" href="#networking-101" title="Link to this heading">#</a></h3>
<p>Understanding how services talk to each other over a network is crucial,
especially when working with tools like OpenTelemetry and Docker Compose. Let’s
break down some key terms:</p>
<ul class="simple">
<li><p><strong>Host:</strong> This is simply the computer or server where a program is running.
It could be your physical laptop, a virtual machine (VM) in the cloud, or
even a container (like those managed by Docker). Think of it as the
“machine”.</p></li>
<li><p><strong>Port:</strong> Imagine a large building (the host machine) with many numbered
doors (ports). When programs need to communicate over a network, they
connect to a specific “door” or port number on the target host. This allows
multiple services to run on the same host without interfering with each
other.</p>
<ul>
<li><p>Ports are numbered from 0 to 65535.</p></li>
<li><p>Many common services have default ports (e.g., web servers often use
port 80 for HTTP and 443 for HTTPS).</p></li>
<li><p>OpenTelemetry’s OTLP protocol, for example, often uses port <code class="docutils literal notranslate"><span class="pre">4317</span></code> (for
gRPC) and <code class="docutils literal notranslate"><span class="pre">4318</span></code> (for HTTP).</p></li>
</ul>
</li>
<li><p><strong>Listening on a Port:</strong> When a service (like an OTel Collector receiver) is
“listening” on a specific port, it means it’s actively waiting at that
numbered door on its host, ready to accept incoming network connections or
data sent specifically to that port number.</p></li>
<li><p><strong>IP Address:</strong> This is like the unique street address for a host machine on
a network. It allows other machines to find it and send data to it. There
are a few special IP addresses:</p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code> (<code class="docutils literal notranslate"><span class="pre">localhost</span></code>):</strong> This special IP address always means
“this machine itself”. When a service listens on <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>, it can
only accept connections originating from the <em>same</em> machine. The
hostname <code class="docutils literal notranslate"><span class="pre">localhost</span></code> usually resolves to this IP address. It’s often
used for local development or services that should only talk to
themselves.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code>:</strong> This special address isn’t a real destination but is used
when configuring a service to listen. It means “listen on <em>all</em>
available network interfaces on this host”. If a host has multiple ways
to be reached (e.g., Wi-Fi with IP <code class="docutils literal notranslate"><span class="pre">192.168.1.5</span></code>, Ethernet with IP
<code class="docutils literal notranslate"><span class="pre">10.0.0.2</span></code>, and the <code class="docutils literal notranslate"><span class="pre">localhost</span></code> interface <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>), a service
listening on <code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code> will accept connections coming into <em>any</em> of
those addresses on the specified port. This is very common for services
(like web servers or the OTel Collector) that need to be reachable from
other machines or containers.</p></li>
</ul>
</li>
<li><p><strong>Endpoint (<code class="docutils literal notranslate"><span class="pre">&lt;HOST&gt;:&lt;PORT&gt;</span></code>):</strong> This combination of a host identifier (an IP
address like <code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code> or <code class="docutils literal notranslate"><span class="pre">192.168.1.5</span></code>, or a hostname like <code class="docutils literal notranslate"><span class="pre">localhost</span></code> or
<code class="docutils literal notranslate"><span class="pre">otel-collector</span></code>) and a port number (<code class="docutils literal notranslate"><span class="pre">4317</span></code>) defines a specific
communication endpoint.</p>
<ul>
<li><p>A <em>server</em> or <em>receiver</em> (like the OTel Collector) listens on an
endpoint (e.g., <code class="docutils literal notranslate"><span class="pre">0.0.0.0:4317</span></code> means listen on port 4317 on all
interfaces).</p></li>
<li><p>A <em>client</em> (like your application sending telemetry) needs to know the
target endpoint to connect to (e.g., <code class="docutils literal notranslate"><span class="pre">192.168.1.10:4317</span></code> or
<code class="docutils literal notranslate"><span class="pre">otel-collector:4317</span></code>).</p></li>
</ul>
</li>
</ul>
</section>
<section id="networking-in-containers-docker-docker-compose">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Networking in Containers (Docker / Docker Compose)</a><a class="headerlink" href="#networking-in-containers-docker-docker-compose" title="Link to this heading">#</a></h3>
<p>When using containers, things work slightly differently, making the following
concepts important:</p>
<ul class="simple">
<li><p><strong>Container Network Isolation:</strong> By default, Docker creates a private
network for containers launched together (e.g., via Docker Compose). Each
container gets its own internal IP address within this private network.
Services running inside a container are initially only reachable by other
containers on the same Docker network, not directly from your host machine.
<code class="docutils literal notranslate"><span class="pre">localhost</span></code> or <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code> <em>inside</em> a container refers to the container
<em>itself</em>, not your host machine.</p></li>
<li><p><strong>Port Mapping:</strong> To access a service running <em>inside</em> a container from
<em>outside</em> (e.g., from your host machine’s browser or another application not
in the same Docker network), you need to map a port from the host machine to
the container port.</p>
<ul>
<li><p>In Docker Compose (<code class="docutils literal notranslate"><span class="pre">docker-compose.yaml</span></code>), this is done using the
<code class="docutils literal notranslate"><span class="pre">ports:</span></code> section.</p></li>
<li><p>Syntax is typically <code class="docutils literal notranslate"><span class="pre">HOST_PORT:CONTAINER_PORT</span></code> (often written as a
string like <code class="docutils literal notranslate"><span class="pre">&quot;4317:4317&quot;</span></code>).</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">ports:</span> <span class="pre">[&quot;8080:80&quot;]</span></code> means traffic sent to port <code class="docutils literal notranslate"><span class="pre">8080</span></code> on your
<em>host machine</em> will be forwarded to port <code class="docutils literal notranslate"><span class="pre">80</span></code> <em>inside</em> the container. If
the OTel Collector inside a container listens on <code class="docutils literal notranslate"><span class="pre">0.0.0.0:4317</span></code>, you
might use <code class="docutils literal notranslate"><span class="pre">ports:</span> <span class="pre">[&quot;4317:4317&quot;]</span></code> to make it reachable via
<code class="docutils literal notranslate"><span class="pre">localhost:4317</span></code> (or <code class="docutils literal notranslate"><span class="pre">&lt;your_host_ip&gt;:4317</span></code>) on your host machine.</p></li>
</ul>
</li>
<li><p><strong>Service Discovery (Service Names):</strong> How do containers within the same
Docker Compose setup find each other? Docker’s networking allows containers
on the same network to find each other using the <em>service names</em> defined in
your <code class="docutils literal notranslate"><span class="pre">docker-compose.yaml</span></code> file.</p>
<ul>
<li><p>If you define a service named <code class="docutils literal notranslate"><span class="pre">otel-collector</span></code> and another named
<code class="docutils literal notranslate"><span class="pre">my-app</span></code> in your <code class="docutils literal notranslate"><span class="pre">docker-compose.yaml</span></code>, the <code class="docutils literal notranslate"><span class="pre">my-app</span></code> container can
typically send data to the collector using the hostname <code class="docutils literal notranslate"><span class="pre">otel-collector</span></code>
and the port the collector is listening on internally.</p></li>
<li><p>Example: Your application code inside the <code class="docutils literal notranslate"><span class="pre">my-app</span></code> container might
configure its OTel exporter to send data to the endpoint
<code class="docutils literal notranslate"><span class="pre">otel-collector:4317</span></code>. Docker automatically resolves <code class="docutils literal notranslate"><span class="pre">otel-collector</span></code> to
the correct internal IP address of the collector container within the
Docker network.</p></li>
</ul>
</li>
</ul>
<p>Understanding these concepts helps demystify how services, especially within
containers, are configured to listen for traffic (<code class="docutils literal notranslate"><span class="pre">0.0.0.0:&lt;port&gt;</span></code>), how they
are made accessible from the outside (<code class="docutils literal notranslate"><span class="pre">ports:</span></code> mapping), and how they
communicate with each other (<code class="docutils literal notranslate"><span class="pre">&lt;service-name&gt;:&lt;port&gt;</span></code>).</p>
</section>
</section>
<section id="anatomy-of-telemetry-flow">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Anatomy of Telemetry Flow</a><a class="headerlink" href="#anatomy-of-telemetry-flow" title="Link to this heading">#</a></h2>
<p>Let’s first see the flow of telemetry data from the application to the
collector. It is key to understand the flow of telemetry data to understand the
configuration of the collector.</p>
<p>In short, the flow is as follows:</p>
<ol class="arabic simple">
<li><p>The application code uses the OpenTelemetry SDK to generate telemetry data.</p></li>
<li><p>The SDK generates telemetry data and sends it to the collector via the OTLP
protocol.</p></li>
<li><p>The collector receives the telemetry data and processes it.</p></li>
<li><p>The collector exports the telemetry data to the backend systems.</p></li>
</ol>
<blockquote>
<div><p>This allows for a separation of concerns, as you can change the backend
systems by reconfiguring the collector without changing the application code.</p>
</div></blockquote>
<pre data-zoom-id="id-b4150d35-2b9e-4046-8c9a-d86234cb7c66" class="mermaid">
        flowchart TB
    %% Application and SDK Layer (From Diagram 1)
    subgraph &quot;Application Layer&quot;
        App[&quot;Application Code&quot;]
        TelemFacade[&quot;Telemetry Facade&quot;]

        subgraph &quot;OpenTelemetry SDK&quot;
            direction TB
            TracerProvider[&quot;TracerProvider&quot;]
            MeterProvider[&quot;MeterProvider&quot;]
            LoggerProvider[&quot;LoggerProvider&quot;]
        end

        subgraph &quot;SDK Processors/Readers&quot;
             direction TB
            BatchSpanProcessor[&quot;BatchSpanProcessor&quot;]
            PeriodicMetricReader[&quot;PeriodicMetricReader&quot;]
            BatchLogProcessor[&quot;BatchLogProcessor&quot;]
        end

        subgraph &quot;SDK Exporters&quot;
             direction TB
            OTLPTraceExporter[&quot;OTLPSpanExporter&quot;]
            OTLPMetricExporter[&quot;OTLPMetricExporter&quot;]
            OTLPLogExporter[&quot;OTLPLogExporter&quot;]
        end
    end

    %% OTLP Protocol Layer (From Diagram 1)
    subgraph &quot;Protocol Layer&quot;
        direction LR
        OTLP[&quot;OTLP/gRPC (4317)&quot;]
        OTLPHTTP[&quot;OTLP/HTTP (4318)&quot;]
    end


    %% Collector Layer (Combining Both Diagrams)
    subgraph &quot;OpenTelemetry Collector&quot;
        direction TB

        subgraph &quot;Collector: Extensions&quot;
            direction TB
            HealthCheck[&quot;Health Check (13133)&quot;]
            PProf[&quot;PProf Profiling (1777)&quot;]
            ZPages[&quot;ZPages Diagnostics (55679)&quot;]
            %% Note: Extensions exist alongside, not directly in data path typically
        end

        subgraph &quot;Collector: Receivers&quot;
            direction TB
            OTLPReceiver[&quot;OTLP Receiver (gRPC/HTTP)&quot;]
        end

        %% Explicit Pipeline Processing (Inspired by Diagram 2)
        subgraph &quot;Collector: Pipeline Processing&quot;
            direction TB

            subgraph &quot;Traces Pipeline Processors&quot;
                direction LR
                style TraceProcess fill:#fff,stroke:#6f42c1,stroke-dasharray: 5 5
                BatchProcessorT[&quot;Batch Processor&quot;] --&gt; MemoryLimiterT[&quot;Memory Limiter&quot;] --&gt; SamplingProcessorT[&quot;Sampling Processor&quot;]
            end

            subgraph &quot;Metrics Pipeline Processors&quot;
                direction LR
                 style MetricsProcess fill:#fff,stroke:#6f42c1,stroke-dasharray: 5 5
                %% Assuming simpler pipeline for metrics based on common configs
                BatchProcessorM[&quot;Batch Processor&quot;] --&gt; MemoryLimiterM[&quot;Memory Limiter&quot;]
            end

             subgraph &quot;Logs Pipeline Processors&quot;
                direction LR
                 style LogsProcess fill:#fff,stroke:#6f42c1,stroke-dasharray: 5 5
                 %% Assuming simpler pipeline for logs
                BatchProcessorL[&quot;Batch Processor&quot;] --&gt; MemoryLimiterL[&quot;Memory Limiter&quot;]
            end
        end


        subgraph &quot;Collector: Exporters&quot;
            direction TB
            DebugExporter[&quot;Debug Exporter&quot;]
            PromExporter[&quot;Prometheus Exporter (8889)&quot;]
            OTLPJaegerExporter[&quot;OTLP Exporter (to Jaeger)&quot;]
        end

    end

    %% Backend Systems Layer (From Diagram 1)
    subgraph &quot;Observability Backends&quot;
        direction LR
        Prometheus[&quot;Prometheus&quot;]
        Jaeger[&quot;Jaeger&quot;]
        Loki[&quot;Loki (Future)&quot;]
        Alerting[&quot;Alerting Systems&quot;]
    end

    %% Connections: App SDK Internal
    App --&gt; TelemFacade
    TelemFacade --&gt; TracerProvider
    TelemFacade --&gt; MeterProvider
    TelemFacade --&gt; LoggerProvider

    TracerProvider --&gt; BatchSpanProcessor
    MeterProvider --&gt; PeriodicMetricReader
    LoggerProvider --&gt; BatchLogProcessor

    BatchSpanProcessor --&gt; OTLPTraceExporter
    PeriodicMetricReader --&gt; OTLPMetricExporter
    BatchLogProcessor --&gt; OTLPLogExporter

    %% Connections: SDK to Protocol
    OTLPTraceExporter --&gt; OTLP
    OTLPMetricExporter --&gt; OTLP
    OTLPLogExporter --&gt; OTLP

    OTLPTraceExporter -.-&gt; OTLPHTTP
    OTLPMetricExporter -.-&gt; OTLPHTTP
    OTLPLogExporter -.-&gt; OTLPHTTP

    %% Connections: Protocol to Collector Receiver
    OTLP --&gt; OTLPReceiver
    OTLPHTTP --&gt; OTLPReceiver

    %% Connections: Receiver to Pipelines (Connect to START of each pipeline processing)
    OTLPReceiver -- &quot;Traces&quot; --&gt; BatchProcessorT
    OTLPReceiver -- &quot;Metrics&quot; --&gt; BatchProcessorM
    OTLPReceiver -- &quot;Logs&quot; --&gt; BatchProcessorL

    %% Connections: Pipeline Processors to Exporters (Connect from END of each pipeline processing)
    SamplingProcessorT -- &quot;Trace Data&quot; --&gt; OTLPJaegerExporter
    SamplingProcessorT -- &quot;Trace Data&quot; --&gt; DebugExporter

    MemoryLimiterM -- &quot;Metric Data&quot; --&gt; PromExporter
    MemoryLimiterM -- &quot;Metric Data&quot; --&gt; DebugExporter

    MemoryLimiterL -- &quot;Log Data&quot; --&gt; DebugExporter
    %% Assuming DebugExporter can handle all signal types

    %% Connections: Collector Exporters to Backends
    PromExporter -- &quot;HTTP Scrape (Pull)&quot; --&gt; Prometheus
    OTLPJaegerExporter -- &quot;OTLP/gRPC (Push)&quot; --&gt; Jaeger
    %% DebugExporter connection to Loki is conceptual/placeholder
    DebugExporter -.-&gt; Loki


    %% Connections: Backends to Alerting
    Prometheus --&gt; Alerting

    %% Styling (From Diagram 1, applied to relevant nodes)
    classDef application fill:#f8d7da,stroke:#dc3545,stroke-width:2px
    classDef sdk fill:#d1e7dd,stroke:#198754,stroke-width:2px
    classDef sdkProcessor fill:#fff3cd,stroke:#ffc107,stroke-width:2px
    classDef sdkExporter fill:#cff4fc,stroke:#0dcaf0,stroke-width:2px
    classDef protocol fill:#e2e3e5,stroke:#6c757d,stroke-width:2px

    classDef collectorReceiver fill:#d0bfff,stroke:#6f42c1,stroke-width:2px
    %% Use distinct IDs for processors if they represent different configurations
    classDef collectorProcessor fill:#ffc107,stroke:#fd7e14,stroke-width:2px
    classDef collectorExporter fill:#20c997,stroke:#198754,stroke-width:2px
    classDef collectorExtension fill:#f8f9fa,stroke:#212529,stroke-width:2px

    classDef backend fill:#adb5bd,stroke:#495057,stroke-width:2px

    %% Apply styles
    class App,TelemFacade application
    class TracerProvider,MeterProvider,LoggerProvider sdk
    class BatchSpanProcessor,PeriodicMetricReader,BatchLogProcessor sdkProcessor
    class OTLPTraceExporter,OTLPMetricExporter,OTLPLogExporter sdkExporter
    class OTLP,OTLPHTTP protocol

    class OTLPReceiver collectorReceiver
    class BatchProcessorT,MemoryLimiterT,SamplingProcessorT,BatchProcessorM,MemoryLimiterM,BatchProcessorL,MemoryLimiterL collectorProcessor
    class PromExporter,OTLPJaegerExporter,DebugExporter collectorExporter
    class HealthCheck,PProf,ZPages collectorExtension

    class Prometheus,Jaeger,Loki,Alerting backend
    </pre><p>At a very high level, we see a pseudo example:</p>
<p>In our code, we might have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Application code exporter - sends TO the collector</span>
<span class="n">otlp_exporter</span> <span class="o">=</span> <span class="n">OTLPSpanExporter</span><span class="p">(</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;http://otel-collector:4317&quot;</span>  <span class="c1"># Collector address</span>
<span class="p">)</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">TracerProvider</span><span class="p">()</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">BatchSpanProcessor</span><span class="p">(</span><span class="n">otlp_exporter</span><span class="p">)</span>
<span class="n">provider</span><span class="o">.</span><span class="n">add_span_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
</pre></div>
</div>
<p>This corresponds to first pointer above “Your application code uses the SDK with
an OTLP exporter to send telemetry to the collector”.</p>
<p>Following this, we see that in our
<a class="reference download internal" download="" href="_downloads/bcd84ad6c936b06f226d4b1b1611d809/collector-config.yaml"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">collector-config.yaml</span></code></span></a> file, the <code class="docutils literal notranslate"><span class="pre">receivers</span></code>
section is configured to receive OTLP data on port 4317 (and 4318 for HTTP). And
indeed our code has endpoints that point to this address:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">otlp_exporter</span> <span class="o">=</span> <span class="n">OTLPSpanExporter</span><span class="p">(</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;http://otel-collector:4317&quot;</span>  <span class="c1"># Collector address</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This means that when we run our application, it will send telemetry data to the
collector at <code class="docutils literal notranslate"><span class="pre">http://otel-collector:4317</span></code>. This corresponds to the 2nd pointer
above “The collector receives this data via its OTLP receiver”.</p>
<p>Next, the 3rd pointer above “The collector processes the data (batching,
filtering, enriching)” will then process this data via batching etc.</p>
<p>Lastly, the 4th pointer above “The collector’s exporters forward the data to
various backends” will then send the data to a Jaeger backend and a Prometheus
backend.”</p>
<p>And in our <a class="reference download internal" download="" href="_downloads/bcd84ad6c936b06f226d4b1b1611d809/collector-config.yaml"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">collector-config.yaml</span></code></span></a> file, we
might have:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collector exporters - send FROM the collector to backends</span>
<span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger:4317</span><span class="w"> </span><span class="c1"># Jaeger backend</span>
<span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:8889</span><span class="w"> </span><span class="c1"># For Prometheus to scrape</span>
</pre></div>
</div>
<section id="generation-phase-application-level-telemetry-instantiation">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">1. Generation Phase: Application-Level Telemetry Instantiation</a><a class="headerlink" href="#generation-phase-application-level-telemetry-instantiation" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Instrumentation Context:</strong> Telemetry generation is initiated within the
instrumented <code class="docutils literal notranslate"><span class="pre">Application</span> <span class="pre">Code</span></code>. This occurs either through manual
instrumentation (explicit API calls to OTel SDK) or automatic
instrumentation (libraries that intercept standard operations like HTTP
requests or DB calls). Central to this is <strong>Context Propagation</strong>, where
metadata (like Trace IDs and Span IDs) is carried across asynchronous
boundaries and process hops, ensuring causal links are maintained.</p></li>
<li><p><strong>SDK Core Components:</strong></p>
<ul class="simple">
<li><p><strong>Providers (<code class="docutils literal notranslate"><span class="pre">TracerProvider</span></code>, <code class="docutils literal notranslate"><span class="pre">MeterProvider</span></code>, <code class="docutils literal notranslate"><span class="pre">LoggerProvider</span></code>):</strong>
These act as factories and registries for their respective signal types.
They manage the SDK’s configuration (e.g., Resource attributes, Sampler
for traces, Views for metrics) and provide access to <code class="docutils literal notranslate"><span class="pre">Tracer</span></code>, <code class="docutils literal notranslate"><span class="pre">Meter</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Logger</span></code> instances.</p></li>
<li><p><strong>API Interfaces (<code class="docutils literal notranslate"><span class="pre">Tracer</span></code>, <code class="docutils literal notranslate"><span class="pre">Meter</span></code>, <code class="docutils literal notranslate"><span class="pre">Logger</span></code>):</strong> These are the
interfaces used by instrumentation code. For example,
<code class="docutils literal notranslate"><span class="pre">Tracer.start_span()</span></code> initiates a trace span, <code class="docutils literal notranslate"><span class="pre">Meter.create_counter()</span></code>
defines a metric instrument, and <code class="docutils literal notranslate"><span class="pre">Logger.emit()</span></code> captures a log record.</p></li>
<li><p><strong>Signal Processors/Readers:</strong></p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">SpanProcessor</span></code> (<code class="docutils literal notranslate"><span class="pre">BatchSpanProcessor</span></code>, <code class="docutils literal notranslate"><span class="pre">SimpleSpanProcessor</span></code>):</strong>
Invoked on span lifecycle events (typically <code class="docutils literal notranslate"><span class="pre">on_start</span></code>, <code class="docutils literal notranslate"><span class="pre">on_end</span></code>).
The <code class="docutils literal notranslate"><span class="pre">BatchSpanProcessor</span></code> accumulates completed spans in a queue and
flushes them periodically or when the queue reaches a certain size
to an associated <code class="docutils literal notranslate"><span class="pre">SpanExporter</span></code>. The <code class="docutils literal notranslate"><span class="pre">SimpleSpanProcessor</span></code> exports
spans immediately upon completion (primarily for debugging).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">MetricReader</span></code> (<code class="docutils literal notranslate"><span class="pre">PeriodicMetricReader</span></code>):</strong> Responsible for
collecting aggregated metric data from all registered instruments at
defined intervals. It pulls delta or cumulative values (depending on
configuration and instrument type) and passes them to an associated
<code class="docutils literal notranslate"><span class="pre">MetricExporter</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">LogRecordProcessor</span></code> (<code class="docutils literal notranslate"><span class="pre">BatchLogRecordProcessor</span></code>):</strong> Similar to the
span processor, it batches log records before exporting them via a
<code class="docutils literal notranslate"><span class="pre">LogRecordExporter</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Data Instantiation Example:</strong> When an incoming HTTP request hits an
instrumented server:</p>
<ol class="arabic simple">
<li><p>Auto-instrumentation (or manual code) extracts any incoming trace
context (Trace ID, Parent Span ID) via context propagation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tracer.start_span()</span></code> creates a new <code class="docutils literal notranslate"><span class="pre">Span</span></code> object, marking the start
time, associating it with the extracted or a new Trace ID, and setting
attributes (HTTP method, URL, etc.). This span becomes the “current”
span in the execution context.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">Counter</span></code> metric instrument (obtained via <code class="docutils literal notranslate"><span class="pre">Meter.create_counter()</span></code>)
might be incremented (<code class="docutils literal notranslate"><span class="pre">counter.add(1)</span></code>). The SDK internally aggregates
this increment.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">LogRecord</span></code> might be emitted via the <code class="docutils literal notranslate"><span class="pre">LoggerProvider</span></code>’s logger,
capturing timestamp, severity, body, and attributes. The current
Trace/Span ID is automatically associated if configured.</p></li>
<li><p>As the request completes, <code class="docutils literal notranslate"><span class="pre">span.end()</span></code> is called, recording the end time
and status. The <code class="docutils literal notranslate"><span class="pre">SpanProcessor</span></code>’s <code class="docutils literal notranslate"><span class="pre">on_end</span></code> method is triggered,
potentially adding the completed span to its export batch.</p></li>
</ol>
</li>
<li><p><strong>SDK Export Configuration:</strong> The application configures specific exporters
(e.g., <code class="docutils literal notranslate"><span class="pre">OTLPSpanExporter</span></code>) and associates them with the corresponding
processors/readers. The exporter is given the target endpoint and
credentials for the OTel Collector or backend.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Rigorous SDK Configuration Example (Conceptual)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opentelemetry.sdk.trace</span><span class="w"> </span><span class="kn">import</span> <span class="n">TracerProvider</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opentelemetry.sdk.trace.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">BatchSpanProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opentelemetry.exporter.otlp.proto.grpc.trace_exporter</span><span class="w"> </span><span class="kn">import</span> <span class="n">OTLPSpanExporter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opentelemetry.sdk.resources</span><span class="w"> </span><span class="kn">import</span> <span class="n">Resource</span>

<span class="c1"># Define resource attributes (applied to all telemetry from this SDK instance)</span>
<span class="n">resource</span> <span class="o">=</span> <span class="n">Resource</span><span class="p">(</span><span class="n">attributes</span><span class="o">=</span><span class="p">{</span> <span class="s2">&quot;service.name&quot;</span><span class="p">:</span> <span class="s2">&quot;my-web-service&quot;</span><span class="p">,</span> <span class="s2">&quot;service.version&quot;</span><span class="p">:</span> <span class="s2">&quot;1.2.3&quot;</span> <span class="p">})</span>

<span class="c1"># Configure OTLP exporter (gRPC to collector)</span>
<span class="c1"># In production: Use secure credentials, proper endpoint resolution.</span>
<span class="n">otlp_exporter</span> <span class="o">=</span> <span class="n">OTLPSpanExporter</span><span class="p">(</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;otel-collector:4317&quot;</span><span class="p">,</span> <span class="c1"># DNS name resolved by container orchestrator</span>
    <span class="n">insecure</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Use TLS in production!</span>
    <span class="c1"># timeout=5, # Specify connection/RPC timeouts</span>
    <span class="c1"># headers=((&quot;auth-header&quot;, &quot;auth-token&quot;),) # Optional metadata/auth</span>
<span class="p">)</span>

<span class="c1"># Configure Batch Span Processor</span>
<span class="c1"># Exports every 5s or when 512 spans are queued, max queue size 2048, export timeout 30s</span>
<span class="n">bsp</span> <span class="o">=</span> <span class="n">BatchSpanProcessor</span><span class="p">(</span>
    <span class="n">otlp_exporter</span><span class="p">,</span>
    <span class="n">schedule_delay_millis</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">max_export_batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">max_queue_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">export_timeout_millis</span><span class="o">=</span><span class="mi">30000</span>
<span class="p">)</span>

<span class="c1"># Setup Tracer Provider</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">TracerProvider</span><span class="p">(</span><span class="n">resource</span><span class="o">=</span><span class="n">resource</span><span class="p">)</span>
<span class="n">provider</span><span class="o">.</span><span class="n">add_span_processor</span><span class="p">(</span><span class="n">bsp</span><span class="p">)</span>

<span class="c1"># Set global provider (usually done once at application startup)</span>
<span class="c1"># opentelemetry.trace.set_tracer_provider(provider)</span>
<span class="c1"># Similar setup for MeterProvider with PeriodicMetricReader and OTLPMetricExporter</span>
<span class="c1"># Similar setup for LoggerProvider with BatchLogRecordProcessor and OTLPLogExporter</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="transmission-phase-otlp-network-protocol">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">2. Transmission Phase: OTLP Network Protocol</a><a class="headerlink" href="#transmission-phase-otlp-network-protocol" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Serialization:</strong> The SDK Exporters serialize the batched Spans, Metric
data points, or Log records into their respective OTLP Protobuf message
types (<code class="docutils literal notranslate"><span class="pre">ExportTraceServiceRequest</span></code>, <code class="docutils literal notranslate"><span class="pre">ExportMetricsServiceRequest</span></code>,
<code class="docutils literal notranslate"><span class="pre">ExportLogsServiceRequest</span></code>). These requests contain the telemetry data along
with the <code class="docutils literal notranslate"><span class="pre">Resource</span></code> information defined in the SDK.</p></li>
<li><p><strong>Transport:</strong></p>
<ul>
<li><p><strong>OTLP/gRPC (Port 4317 default):</strong> Uses bidirectional streaming RPCs
over HTTP/2. This is generally more efficient for continuous telemetry
streams due to persistent connections and multiplexing.</p></li>
<li><p><strong>OTLP/HTTP (Port 4318 default):</strong> Uses standard HTTP/1.1 POST requests
with Protobuf or JSON payloads (<code class="docutils literal notranslate"><span class="pre">Content-Type:</span> <span class="pre">application/protobuf</span></code> or
<code class="docutils literal notranslate"><span class="pre">application/json</span></code>). Each request typically contains one batch. Can be
easier to proxy and inspect but may have higher overhead per batch.</p></li>
</ul>
</li>
<li><p><strong>Network Transit:</strong> The serialized OTLP messages travel across the network
from the application container/host to the Collector’s designated listening
address and port. Network latency, bandwidth, and potential packet loss are
factors during this stage.</p></li>
</ul>
</section>
<section id="reception-phase-collector-data-ingestion">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">3. Reception Phase: Collector Data Ingestion</a><a class="headerlink" href="#reception-phase-collector-data-ingestion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Listener Binding:</strong> The OTel Collector’s <code class="docutils literal notranslate"><span class="pre">OTLP</span> <span class="pre">Receiver</span></code> binds to the
specified network interface and port (e.g., <code class="docutils literal notranslate"><span class="pre">0.0.0.0:4317</span></code>). Binding to
<code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code> allows it to accept connections directed to any IP address
assigned to the Collector’s host or container.</p></li>
<li><p><strong>Connection Handling &amp; Deserialization:</strong> The receiver component handles
incoming network connections (TCP for gRPC, TCP for HTTP). It reads the byte
stream, identifies the OTLP request type, and deserializes the Protobuf
messages into the Collector’s internal representation (<code class="docutils literal notranslate"><span class="pre">pdata</span></code> - Pipeline
Data format, which closely mirrors OTLP). This involves parsing the Protobuf
structure and validating mandatory fields. Errors during parsing (e.g.,
corrupted data) typically result in the request being rejected and an error
logged by the Collector.</p></li>
<li><p><strong>Signal Identification:</strong> The receiver intrinsically knows the signal type
based on the specific OTLP RPC endpoint invoked (for gRPC) or the HTTP POST
URL path (for HTTP), as well as the Protobuf message type itself.</p></li>
</ul>
</section>
<section id="processing-phase-collector-pipeline-execution">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">4. Processing Phase: Collector Pipeline Execution</a><a class="headerlink" href="#processing-phase-collector-pipeline-execution" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Pipeline Routing (Fan-out):</strong> Upon successful deserialization, the OTLP
receiver forwards the <code class="docutils literal notranslate"><span class="pre">pdata</span></code> objects to the input stage of <em>all</em> pipelines
declared in the <code class="docutils literal notranslate"><span class="pre">service.pipelines</span></code> section of its configuration that list
this receiver as an input. A single receiver can feed multiple pipelines
(e.g., traces to <code class="docutils literal notranslate"><span class="pre">traces</span></code> pipeline, metrics to <code class="docutils literal notranslate"><span class="pre">metrics</span></code> pipeline).</p></li>
<li><p><strong>Sequential Processor Execution:</strong> Within each pipeline, the <code class="docutils literal notranslate"><span class="pre">pdata</span></code>
objects flow sequentially through the defined list of processors. The order
specified in the configuration
(<code class="docutils literal notranslate"><span class="pre">service.pipelines.&lt;pipeline_name&gt;.processors</span></code>) is strictly maintained.
Each processor receives data from the previous one (or the receiver),
performs its function, and passes the (potentially modified) data to the
next processor.</p></li>
<li><p><strong>Processor Deep Dive:</strong></p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">memory_limiter</span></code>:</strong> Periodically checks the Collector’s memory usage
(Go runtime’s <code class="docutils literal notranslate"><span class="pre">ReadMemStats</span></code>). If usage exceeds a <code class="docutils literal notranslate"><span class="pre">limit_mib</span></code> threshold,
it begins refusing data (<code class="docutils literal notranslate"><span class="pre">check()</span></code> fails). If usage exceeds a higher
<code class="docutils literal notranslate"><span class="pre">spike_limit_mib</span></code>, it forces garbage collection and refuses data more
aggressively. When refusing data, it signals backpressure up the
pipeline, potentially causing receivers to drop data or signal errors
back to the client SDK (which might trigger retries). It aims to prevent
OutOfMemory (OOM) kills.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">batch</span></code>:</strong> Buffers telemetry items (spans, metrics, logs) based on
<code class="docutils literal notranslate"><span class="pre">timeout</span></code> (e.g., 1 second) or <code class="docutils literal notranslate"><span class="pre">send_batch_size</span></code> (e.g., 8192 items).
Whichever threshold is hit first triggers the formation and forwarding
of a batch to the next processor. This amortizes the overhead of
subsequent processing and export steps.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">resource</span></code>:</strong> Ensures specific resource attributes exist or modifies
them. Useful for standardizing attributes across different sources.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">attributes</span></code>:</strong> Allows manipulation (insertion, update, upsert,
deletion, hashing) of attributes on individual spans, logs, or metric
data points based on configurable rules.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">filter</span></code>:</strong> Includes or excludes telemetry items based on their
attributes or properties (e.g., drop debug logs, keep only spans with
errors).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">spanmetrics</span></code>:</strong> Generates metrics (request counts, latencies) <em>from</em>
span data. Requires both traces and metrics pipelines.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">tail_sampling</span></code>:</strong> (More complex) Buffers all spans belonging to a
trace for a certain period. Once the trace is complete (or timeout
occurs), it evaluates policies (e.g., keep if error, keep if long
duration, probabilistic) across the <em>entire trace</em> before deciding
whether to forward it or drop it. Requires significant memory.</p></li>
</ul>
</li>
<li><p><strong>Pipeline Independence:</strong> Each pipeline (traces, metrics, logs) operates
largely independently, often utilizing separate goroutines and queues,
ensuring that high volume or processing latency in one signal type does not
block others.</p></li>
</ul>
</section>
<section id="exportation-phase-data-egress-to-backends">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">5. Exportation Phase: Data Egress to Backends</a><a class="headerlink" href="#exportation-phase-data-egress-to-backends" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Exporter Invocation (Fan-out):</strong> Once data traverses all processors in a
pipeline, it is passed to <em>all</em> exporters listed for that pipeline
(<code class="docutils literal notranslate"><span class="pre">service.pipelines.&lt;pipeline_name&gt;.exporters</span></code>). The Collector fans out the
data concurrently to each configured exporter.</p></li>
<li><p><strong>Exporter Mechanisms &amp; Reliability:</strong></p>
<ul class="simple">
<li><p><strong>Push Exporters (e.g., <code class="docutils literal notranslate"><span class="pre">otlp</span></code>, <code class="docutils literal notranslate"><span class="pre">otlphttp</span></code>, <code class="docutils literal notranslate"><span class="pre">jaeger</span></code>):</strong> Actively
establish connections and send data to the configured backend endpoint.
These exporters typically incorporate internal queuing, retry logic
(with exponential backoff, jitter, and configurable maximum
intervals/elapsed time), and potentially compression (<code class="docutils literal notranslate"><span class="pre">gzip</span></code>). If
retries are exhausted (e.g., backend remains unavailable), data is
typically dropped, and errors are logged. Success or failure of one
exporter does not affect others operating on the same data batch.</p></li>
<li><p><strong>Pull Exporters (e.g., <code class="docutils literal notranslate"><span class="pre">prometheus</span></code>):</strong> Do not initiate connections.
They maintain an internal cache of the latest metric data received from
the pipeline. They expose an HTTP endpoint (e.g., <code class="docutils literal notranslate"><span class="pre">:8889/metrics</span></code>). When
an external system (Prometheus server) sends an HTTP GET request (a
scrape) to this endpoint, the exporter formats the cached metrics into
the Prometheus exposition format and returns it in the HTTP response.
The onus of scheduling, retries, and handling scrape failures lies with
the Prometheus server.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">debug</span></code> Exporter:</strong> Simply logs the received <code class="docutils literal notranslate"><span class="pre">pdata</span></code> objects to the
Collector’s configured output (console or file) at a specified verbosity
level. It performs no network operations or retries.</p></li>
</ul>
</li>
<li><p><strong>Configuration Example (<code class="docutils literal notranslate"><span class="pre">exporters</span></code> block):</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span><span class="w"> </span><span class="c1"># Sending traces via OTLP/gRPC to Jaeger</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger-collector.observability.svc.cluster.local:4317</span><span class="w"> </span><span class="c1"># K8s service DNS</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span>
<span class="w">            </span><span class="nt">insecure</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"> </span><span class="c1"># Use CA certs in production</span>
<span class="w">            </span><span class="c1"># ca_file: /etc/certs/ca.pem</span>
<span class="w">        </span><span class="nt">sending_queue</span><span class="p">:</span>
<span class="w">            </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">            </span><span class="nt">num_consumers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"> </span><span class="c1"># Parallel connections/requests</span>
<span class="w">            </span><span class="nt">queue_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span><span class="w"> </span><span class="c1"># Max batches buffered in memory</span>
<span class="w">        </span><span class="nt">retry_on_failure</span><span class="p">:</span>
<span class="w">            </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">            </span><span class="nt">initial_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5s</span>
<span class="w">            </span><span class="nt">max_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">            </span><span class="nt">max_elapsed_time</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5m</span><span class="w"> </span><span class="c1"># Stop retrying after 5 minutes</span>

<span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span><span class="w"> </span><span class="c1"># Exposing metrics for Prometheus scrape</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:8889</span>
<span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my_application</span>
<span class="w">        </span><span class="nt">const_labels</span><span class="p">:</span><span class="w"> </span><span class="c1"># Add labels to all exposed metrics</span>
<span class="w">            </span><span class="nt">cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">prod-us-east-1</span>

<span class="w">    </span><span class="nt">debug</span><span class="p">:</span>
<span class="w">        </span><span class="nt">verbosity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">detailed</span><span class="w"> </span><span class="c1"># Options: basic, normal, detailed</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="storage-analysis-phase-backend-systems">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">6. Storage &amp; Analysis Phase: Backend Systems</a><a class="headerlink" href="#storage-analysis-phase-backend-systems" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Data Persistence:</strong> Backend systems (Jaeger, Prometheus, Loki, etc.)
receive the data from Collector exporters (or scrape it). They parse the
data and store it in their optimized formats:</p>
<ul>
<li><p><strong>Jaeger:</strong> Stores trace data, typically indexed by Trace ID, service
name, operation, timestamps, and tags for efficient querying of
individual traces and dependency analysis.</p></li>
<li><p><strong>Prometheus:</strong> Stores metrics as time series (timestamp-value pairs
identified by metric name and label sets), optimized for time-based
aggregation and querying using PromQL. Cardinality (number of unique
label combinations) is a key consideration.</p></li>
<li><p><strong>Loki/Elasticsearch:</strong> Store log records, indexed by timestamps,
labels/metadata, and potentially log content for full-text search and
log aggregation.</p></li>
</ul>
</li>
<li><p><strong>Querying &amp; Visualization:</strong> These backends provide query languages (Jaeger
API, PromQL, LogQL) and often UIs (Jaeger UI, Grafana, Kibana) to search,
visualize, and analyze the stored telemetry, enabling debugging, performance
monitoring, and dashboarding.</p></li>
<li><p><strong>Alerting:</strong> Systems like Prometheus (with Alertmanager) continuously
evaluate rules against metric data, triggering alerts based on defined
thresholds or conditions.</p></li>
</ul>
</section>
<section id="systemic-considerations">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">7. Systemic Considerations</a><a class="headerlink" href="#systemic-considerations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Resource Consumption:</strong> The Collector itself consumes CPU, memory, and
network bandwidth. Processors like <code class="docutils literal notranslate"><span class="pre">tail_sampling</span></code> or exporters with large
queues can significantly increase memory usage. High data volume taxes all
resources.</p></li>
<li><p><strong>Configuration Management:</strong> Correct and consistent configuration across
the SDK, Collector, and Backends is paramount. Errors in endpoint addresses,
protocols, TLS settings, or pipeline definitions are common failure points.</p></li>
<li><p><strong>Observability of Observability:</strong> Collector extensions like
<code class="docutils literal notranslate"><span class="pre">health_check</span></code>, <code class="docutils literal notranslate"><span class="pre">pprof</span></code>, and <code class="docutils literal notranslate"><span class="pre">zpages</span></code>, along with the Collector’s own
metrics (if scraped), are essential for monitoring the health and
performance of the telemetry pipeline itself.</p></li>
</ul>
</section>
</section>
<section id="anatomy-of-telemetry-flow-deep-research-from-openai-we-keep-it">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">Anatomy of Telemetry Flow (Deep Research From OpenAI, We Keep It)</a><a class="headerlink" href="#anatomy-of-telemetry-flow-deep-research-from-openai-we-keep-it" title="Link to this heading">#</a></h2>
<p>Even though below has duplicate explanations when compared to the earlier
section, we keep it because it is a deep research from OpenAI and we want to
keep it for future reference.</p>
<section id="instrumentation-in-the-application-opentelemetry-sdk">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">Instrumentation in the Application (OpenTelemetry SDK)</a><a class="headerlink" href="#instrumentation-in-the-application-opentelemetry-sdk" title="Link to this heading">#</a></h3>
<p>OpenTelemetry instrumentation begins in the application code itself. Using the
OTel SDK (e.g. the Python SDK), developers instrument their application to
capture <strong>traces</strong>, <strong>metrics</strong>, and <strong>logs</strong>. Each category of telemetry is
handled as follows:</p>
<ul class="simple">
<li><p><strong>Traces:</strong> The application code uses the OpenTelemetry API to create spans
for operations (manually or via auto-instrumentation). These spans are
started and ended around sections of code, forming a trace (a distributed
transaction). The OTel <strong>Tracer</strong> records span data (names, timings,
attributes, context links) and maintains parent-child relationships (using
context propagation) so that traces are coherent. When spans end, the SDK
queues them for export via a span processor (commonly a <code class="docutils literal notranslate"><span class="pre">BatchSpanProcessor</span></code>
that buffers spans and sends in batches)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=Batch%20Processor">OTel Collector - Blog by Roman Glushko</a>).
Each span includes resource metadata (like service name) that identifies the
source application. The spans are then transmitted out of the process by an
<strong>OTLP exporter</strong> configured in the SDK, which sends trace data to the
OpenTelemetry Collector endpoint.</p></li>
<li><p><strong>Metrics:</strong> The application defines instruments (counter, histogram, etc.)
via the OTel <strong>Meter</strong> API. As the app runs, these instruments record
measurements (e.g. HTTP request counts, durations, etc.). The OTel SDK
aggregates and periodically collects these measurements. A <strong>MetricReader</strong>
(often a <code class="docutils literal notranslate"><span class="pre">PeriodicExportingMetricReader</span></code>) is set up with an <strong>OTLP metric
exporter</strong> to push the aggregated metrics at intervals. This means metrics
data is exported on a schedule (e.g. every 30s or 60s) rather than
immediately on each update, to reduce overhead. Each metric data point is
tagged with resource attributes (service, host, etc.) similar to traces, so
that metrics can be attributed to the right service instance.</p></li>
<li><p><strong>Logs:</strong> Application logs can be integrated into OpenTelemetry by using the
logging SDK (or instrumentation). For example, the Python OTel SDK offers a
logging handler or <code class="docutils literal notranslate"><span class="pre">LoggingInstrumentor</span></code> that captures logs and attaches
tracing context (so logs can be correlated with traces)
(<a class="reference external" href="https://cloud.google.com/trace/docs/setup/python-ot#:~:text=logHandler%20%3D%20logging,JsonFormatter">Python instrumentation sample  |  Cloud Trace  |  Google Cloud</a>).
These logs can then be exported via an <strong>OTLP log exporter</strong> to the
Collector. In practice, logs are emitted through the standard logging
framework (e.g. Python’s <code class="docutils literal notranslate"><span class="pre">logging</span></code> module), and the OTel SDK’s handler
intercepts and translates them into OTel log records. Like traces and
metrics, resource attributes (service name, etc.) are attached to log
records. The logs are then sent out via OTLP to the collector.</p></li>
</ul>
<p><strong>Separation of concerns:</strong> The OpenTelemetry SDK in the application is
responsible for <em>generating</em> and <em>formatting</em> telemetry data, but it does
minimal processing beyond that. It quickly forwards the data out-of-process
using OTLP, keeping the application overhead low. The heavy lifting (batching,
retrying, data transformation) is deferred to the Collector. This separation
ensures your application isn’t tightly coupled to any specific backend. You can
change where data is sent by reconfiguring the collector, without modifying
application code
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=from%20any%20specific%20observability%20backend,locked%20into%20a%20single%20platform">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=If%20you%20ever%20decide%20to,not%20your%20entire%20application%20codebase">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
In short, the app’s SDK acts as a <strong>telemetry source</strong>, and the Collector will
act as the <strong>telemetry pipeline</strong> and exporter. This decoupling not only
prevents vendor lock-in
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=Direct%20telemetry%20reporting%20or%20using,experiment%20with%20multiple%20backends%20simultaneously">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>),
but also improves reliability by offloading complex processing from the app to
the Collector.</p>
</section>
<section id="telemetry-export-via-otlp-opentelemetry-protocol">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Telemetry Export via OTLP (OpenTelemetry Protocol)</a><a class="headerlink" href="#telemetry-export-via-otlp-opentelemetry-protocol" title="Link to this heading">#</a></h3>
<p>When the SDK exports data, it uses OTLP – the OpenTelemetry Protocol – to send
traces, metrics, and logs to the Collector. <strong>OTLP</strong> is a standardized binary
protocol (based on Protobuf) for transmitting telemetry data between components
of the OpenTelemetry ecosystem
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=OTLP%20is%20a%20telemetry%20data,forwarders%2C%20and%20various%20observability%20backends">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>).
It was designed to be general-purpose and vendor-agnostic so that any
OTLP-compatible backend or collector can understand the data. Some key points
about OTLP:</p>
<ul class="simple">
<li><p><strong>Transports:</strong> OTLP supports both gRPC and HTTP as transport mechanisms.
The default “native” mode is gRPC (commonly on port 4317), which provides
efficient, bidirectional streaming. However, an HTTP/JSON or HTTP/Protobuf
mode (often on port 4318 for JSON or binary protobuf over HTTP) is also
supported for compatibility with systems where gRPC might not be feasible
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=,gRPC%20may%20not%20be%20ideal">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>).
In practice, many SDKs default to gRPC. For example, the OTel Java agent by
default expects to send to a gRPC endpoint (4317). If the Collector is only
listening on HTTP (4318), you must configure the SDK exporter to use
HTTP/protobuf instead
(<a class="reference external" href="https://stackoverflow.com/questions/72099467/error-io-opentelemetry-exporter-internal-grpc-okhttpgrpcexporter#:~:text=opentelemetry,Dotel.exporter.otlp.protocol%3Dhttp%2Fprotobuf">open telemetry - ERROR io.opentelemetry.exporter.internal.grpc.OkHttpGrpcExporter - Stack Overflow</a>).
This is a common pitfall – <strong>the protocol (gRPC vs HTTP) and port must match
between the SDK and Collector</strong>. Ensuring the endpoint URI (and setting like
<code class="docutils literal notranslate"><span class="pre">OTEL_EXPORTER_OTLP_PROTOCOL</span></code>) is correct will avoid connectivity issues.</p></li>
<li><p><strong>Efficiency:</strong> OTLP is built for high throughput and low latency. It uses
compact Protobuf encoding and can batch multiple telemetry items in one
request, which reduces overhead
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=,gRPC%20may%20not%20be%20ideal">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=4,delivery">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
For example, the SDK’s batch processor will bundle many spans or metrics
into one OTLP message. OTLP’s design also allows extensibility (adding new
fields or signal types) without breaking compatibility
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=but%20it%20also%20supports%20,gRPC%20may%20not%20be%20ideal">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>).</p></li>
<li><p><strong>Interoperability:</strong> By using OTLP, all components speak the same language.
The Python SDK sends OTLP, the Collector receives OTLP on its OTLP receiver,
and the Collector can even forward OTLP to another Collector or backend.
This eliminates the need for custom translation between formats in most
cases. In our scenario, the application sends OTLP to the Collector, and the
Collector’s exporters will send OTLP to Jaeger and expose Prometheus metrics
– everything stays in open standards.</p></li>
</ul>
<p><strong>OTLP Endpoints and configuration:</strong> In the given setup, the application likely
knows where to send OTLP data via environment variables or code configuration.
For example, one might set <code class="docutils literal notranslate"><span class="pre">OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317</span></code>
(for gRPC) or <code class="docutils literal notranslate"><span class="pre">http://localhost:4318</span></code> (for HTTP) depending on the Collector’s
config. The Collector’s default OTLP receiver listens on <code class="docutils literal notranslate"><span class="pre">0.0.0.0:4317</span></code> for gRPC
and <code class="docutils literal notranslate"><span class="pre">0.0.0.0:4318</span></code> for HTTP
(<a class="reference external" href="https://medium.com/jaegertracing/introducing-native-support-for-opentelemetry-in-jaeger-eb661be8183c#:~:text=Notice%20that%20compared%20to%20the,previous%20releases">Introducing native support for OpenTelemetry in Jaeger | by Yuri Shkuro | JaegerTracing | Medium</a>).
It’s possible to enable both protocols to be safe. If these endpoints don’t line
up (e.g. SDK sends gRPC to 4317 but collector only enabled 4318 HTTP, or vice
versa), telemetry will not flow. A common example is a “connection refused”
error if, say, the SDK tries gRPC on 4317 but the collector isn’t listening
there. Always double-check that the SDK’s configured protocol and the
Collector’s <code class="docutils literal notranslate"><span class="pre">otlp</span></code> receiver settings match
(<a class="reference external" href="https://stackoverflow.com/questions/72099467/error-io-opentelemetry-exporter-internal-grpc-okhttpgrpcexporter#:~:text=opentelemetry,Dotel.exporter.otlp.protocol%3Dhttp%2Fprotobuf">open telemetry - ERROR io.opentelemetry.exporter.internal.grpc.OkHttpGrpcExporter - Stack Overflow</a>).</p>
</section>
<section id="the-opentelemetry-collectors-role-and-components">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">The OpenTelemetry Collector’s Role and Components</a><a class="headerlink" href="#the-opentelemetry-collectors-role-and-components" title="Link to this heading">#</a></h3>
<p>The OpenTelemetry Collector is a standalone service that sits between the
instrumented applications and the final observability backends. It acts as a
<strong>telemetry pipeline</strong> that receives data, processes it, and exports it onward.
This provides a centralized place to implement batching, retries, filtering, and
to fan-out data to multiple destinations. The Collector is often run as an agent
(one per host or container) or as a gateway (one per cluster), but in all cases
its architecture is the same. It’s essentially a <strong>vendor-agnostic proxy</strong> that
decouples your app from specific backends
(<a class="reference external" href="https://uptrace.dev/opentelemetry/collector#:~:text=OpenTelemetry%20Collector%20serves%20as%20a,such%20as%20Uptrace%20or%20Jaeger">Getting Started with the OpenTelemetry Collector | Uptrace</a>).</p>
<p><strong>Collector Configuration Overview:</strong> The Collector is configured via pipelines
for each signal (traces, metrics, logs). Each pipeline defines a set of
<strong>receivers</strong> (entry points for data), <strong>processors</strong> (intermediate
transformations), and <strong>exporters</strong> (destinations for data). Additionally,
<strong>extensions</strong> provide auxiliary capabilities (health checks, monitoring, etc.)
for the Collector process itself. Below we break down each component in the
context of the provided config:</p>
</section>
<section id="receivers-otlp-receiver-grpc-http">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Receivers – OTLP Receiver (gRPC/HTTP)</a><a class="headerlink" href="#receivers-otlp-receiver-grpc-http" title="Link to this heading">#</a></h3>
<p>Receivers are how data gets <strong>into</strong> the Collector. In our setup, we have the
<code class="docutils literal notranslate"><span class="pre">otlp</span></code> receiver enabled for both gRPC and HTTP protocols. This means the
Collector opens up ports (e.g. 4317 for gRPC, 4318 for HTTP by default) and
<strong>listens for incoming OTLP data</strong>
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20OTel%20Collector%20provides%20HTTP,for%20instrumented%20services%20to%20connect">OTel Collector - Blog by Roman Glushko</a>).
The instrumented Python application, which is exporting via OTLP, will connect
to this receiver.</p>
<ul class="simple">
<li><p>The OTLP receiver understands the OTLP protocol and can ingest all three
signal types (trace spans, metric points, log records) multiplexed over the
same connection. As telemetry arrives, the receiver quickly parses the
Protobuf payload into the Collector’s internal data model. According to the
Collector’s design, it then routes each data item to the appropriate
pipeline by signal type
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=1,components">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
(traces to the traces pipeline, etc.).</p></li>
<li><p>Because both gRPC and HTTP are enabled, the app can use either. gRPC is more
efficient (streaming, less overhead per message), whereas OTLP/HTTP might be
used if gRPC isn’t feasible (e.g. some environments with proxies). In either
case, the receiver ensures the data ends up in the same format internally.
From the perspective of the Collector’s pipeline, it doesn’t matter which
transport was used – the data is now just “OTLP spans” or “OTLP metrics” in
memory.</p></li>
<li><p><strong>Example:</strong> If the app sends a batch of 100 spans via gRPC, the OTLP gRPC
receiver accepts that batch and passes those spans into the traces pipeline.
If another app sends metrics via an HTTP POST to <code class="docutils literal notranslate"><span class="pre">/v1/metrics</span></code> (the
OTLP/HTTP endpoint), the receiver will accept and push those metrics into
the metrics pipeline. This flexibility allows multiple apps and protocols to
feed a single Collector. (Note: Other receivers exist for different
protocols – e.g. Jaeger, Zipkin, Prometheus scraping, etc. – which the
Collector could also run simultaneously
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=In%20addition%20to%20the%20OTel,Ray%2C%20StatsD%2C%20Prometheus%20protocols">OTel Collector - Blog by Roman Glushko</a>)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=Metrics%20Scrapping">OTel Collector - Blog by Roman Glushko</a>).
In our case we rely solely on OTLP for inbound telemetry, simplifying the
setup.)</p></li>
</ul>
</section>
<section id="processors-batch-memory-limiter-resource">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Processors – Batch, Memory Limiter, Resource</a><a class="headerlink" href="#processors-batch-memory-limiter-resource" title="Link to this heading">#</a></h3>
<p>After reception, telemetry data flows through the configured <strong>processors</strong> in
the pipeline. Processors in OpenTelemetry Collector are optional components that
can modify or manage the data stream in-flight (for optimization or enrichment)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=2,it%20for%20storage%20and%20analysis">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
The provided configuration uses three key processors:</p>
<ul class="simple">
<li><p><strong>Batch Processor:</strong> The batch processor is almost always used in production
pipelines. It <strong>accumulates telemetry data and emits it in batches</strong> rather
than one item at a time. This improves throughput and reduces CPU/network
overhead by amortizing the cost of exporting data
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=4,delivery">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
For example, instead of making a separate export call for every single span,
the batch processor might group 100 spans or wait up to a timeout (e.g. 5
seconds) before sending, whichever comes first. Batching also increases the
likelihood of compressing data efficiently and using fewer requests. The
batch processor is analogous to the SDK’s batch span processor (and indeed
uses similar settings like max batch size and flush timeout)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=Batch%20Processor">OTel Collector - Blog by Roman Glushko</a>),
but it operates in the Collector for all signals (traces, metrics, logs). By
buffering data briefly, it can also smooth out bursts of telemetry – if the
app suddenly emits a spike of 1000 spans, the batcher can chunk these and
not overwhelm the exporter or network all at once. <strong>Reliability:</strong> Combined
with retries (either via built-in exporter retry mechanisms or a retry
processor), batching helps ensure data is delivered even if the backend is
temporarily slow or unavailable
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=4,delivery">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
Overall, this processor optimizes performance and network usage.</p></li>
<li><p><strong>Memory Limiter Processor:</strong> The memory limiter is a <strong>safety valve</strong> for
the Collector’s memory usage. The Collector processes a potentially huge
volume of data and could consume too much memory (leading to OOM) if
incoming data outpaces the ability to export it. The memory_limiter
processor monitors the process memory and if usage exceeds a configured
<strong>soft limit</strong>, it will start dropping incoming data (i.e. applying
backpressure)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20memory%20limiter%20checks%20service,any%20of%20the%20defined%20limits">OTel Collector - Blog by Roman Glushko</a>).
Dropping data is unfortunate but preferable to the Collector crashing
completely. If memory usage hits an even higher <strong>hard limit</strong>, the memory
limiter forces a garbage collection and still rejects new data until memory
is under control
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20memory%20limiter%20checks%20service,any%20of%20the%20defined%20limits">OTel Collector - Blog by Roman Glushko</a>).
In effect, when the soft limit is breached, the Collector tells senders “I
can’t take more right now” (for protocols like gRPC, this backpressure
propagates by not reading new messages, causing the SDK exporter to block or
retry). The design assumes that SDKs or agents will handle these rejections
gracefully – e.g. by retrying later
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20memory%20limiter%20can%20be,down%20to%20retries%20with%20backoff">OTel Collector - Blog by Roman Glushko</a>).
This processor greatly improves stability under load: instead of the
Collector running out of memory and crashing (losing all buffered
telemetry), it sheds load in a controlled way. Best practice is to place the
memory_limiter as the first processor in each pipeline, right after
receivers, so it can start dropping data early and signal backpressure
upstream as soon as memory limits are hit
(<a class="reference external" href="https://docs.splunk.com/observability/gdi/opentelemetry/components/memory-limiter-processor.html#:~:text=Define%20the%20,memory_limiter">Memory Limiter processor — Splunk Observability Cloud documentation</a>)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20memory%20limiter%20can%20be,down%20to%20retries%20with%20backoff">OTel Collector - Blog by Roman Glushko</a>).
In a healthy system, this processor might never actually drop data; it’s
there as a guardrail for overload scenarios.</p></li>
<li><p><strong>Resource Processor:</strong> The resource processor modifies or adds <strong>resource
attributes</strong> on telemetry. Resource attributes are metadata like service
name, service version, host name, region/zone, etc., that apply to all
spans/metrics/logs from a given source. In our pipelines, the resource
processor can ensure that a <code class="docutils literal notranslate"><span class="pre">service.name</span></code> attribute is present and set to a
specific value (for example, upserting “service.name=my-python-app”). It can
also add other dimensions such as cluster or namespace, possibly drawn from
environment variables or other existing attributes. This is useful when the
instrumentation SDK did not supply certain resource info, or if you want to
standardize/override it at the Collector level
(<a class="reference external" href="https://docs.splunk.com/observability/gdi/opentelemetry/components/resource-processor.html#:~:text=The%20resource%20processor%20is%20an,with%20pipelines%20for%20more%20information">Resource processor — Splunk Observability Cloud documentation</a>).
For instance, if the app forgot to set service name, you can configure the
resource processor to insert it, preventing the dreaded “unknown_service”
label in backends. The resource processor can also delete or rename resource
attributes (for example, stripping out an unwanted tag). In summary, this
processor <strong>enriches telemetry with consistent metadata</strong> that is crucial
for querying and filtering in observability backends. (If no changes are
needed – e.g. the SDK already set all desired resource info – this processor
effectively does nothing. It’s optional but often used to enforce tagging
conventions.)</p></li>
</ul>
<p>Together, these processors help ensure the telemetry is well-formed and the
pipeline is robust. The <strong>order</strong> is typically memory_limiter first (to control
load early), then batch (to group data for efficiency), then resource (to tag
everything before export). In some configs, resource might come earlier if you
want to ensure even the batching is grouped by certain resource, but ordering
can vary. In our case, all three are present in each pipeline
(traces/metrics/logs), which is a common best practice setup.</p>
</section>
<section id="exporters-otlp-to-jaeger-prometheus-debug">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Exporters – OTLP (to Jaeger), Prometheus, Debug</a><a class="headerlink" href="#exporters-otlp-to-jaeger-prometheus-debug" title="Link to this heading">#</a></h3>
<p>After processing, the data reaches the <strong>exporters</strong> – these are the components
that send data <em>out</em> of the Collector to various backends or outputs. Exporters
are configured per pipeline. Let’s break down the ones in use:</p>
<ul>
<li><p><strong>OTLP Exporter (to Jaeger):</strong> In the traces pipeline, an <code class="docutils literal notranslate"><span class="pre">otlp</span></code> exporter is
configured to send data to Jaeger. Modern Jaeger versions (&gt;= 1.35) can
natively ingest OTLP trace data
(<a class="reference external" href="https://medium.com/jaegertracing/introducing-native-support-for-opentelemetry-in-jaeger-eb661be8183c#:~:text=With%20this%20new%20capability%2C%20it,1">Introducing native support for OpenTelemetry in Jaeger | by Yuri Shkuro | JaegerTracing | Medium</a>),
so instead of using a Jaeger-specific exporter, we use the OTLP exporter
pointing at the Jaeger collector’s OTLP endpoint. For example, the config
might be:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp/jaeger</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger:4317</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span>
<span class="w">            </span><span class="nt">insecure</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>This config (illustrated above) defines an OTLP exporter named “otlp/jaeger”
sending to host <code class="docutils literal notranslate"><span class="pre">jaeger</span></code> on port <code class="docutils literal notranslate"><span class="pre">4317</span></code> (gRPC) without TLS
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=exporters%3A%20otlp%2Fjaeger%3A%20endpoint%3A%20jaeger%3A4317%20tls%3A,insecure%3A%20true">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
When trace data reaches this exporter, the Collector opens a gRPC connection
to the Jaeger backend and transmits the spans in OTLP format. Jaeger’s OLTP
receiver (enabled by <code class="docutils literal notranslate"><span class="pre">COLLECTOR_OTLP_ENABLED=true</span></code> on Jaeger) will receive
these and store them (e.g. in memory or Elasticsearch, depending on Jaeger
setup). Essentially, the Collector is <strong>pushing trace data</strong> to Jaeger. This
exporter is specific to trace data (since Jaeger is a tracing system – it
will ignore any metrics/logs even if they were accidentally sent). By using
OTLP, we maintain a clean, standard protocol all the way into Jaeger,
avoiding any translation layer. <em>(In older setups, one might use a <code class="docutils literal notranslate"><span class="pre">jaeger</span></code>
exporter that converts OTLP to Jaeger’s Thrift format, but that’s no longer
necessary
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=Here%27s%20an%20sample%20configuration%20exporting,to%20a%20local%20Jaeger%20instance">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).)</em></p>
</li>
<li><p><strong>Prometheus Exporter:</strong> In the metrics pipeline, the exporter is
<code class="docutils literal notranslate"><span class="pre">prometheus</span></code>. This exporter works differently from most others: it doesn’t
push data to an external system. Instead, it turns the Collector into a
<strong>Prometheus metrics endpoint</strong> that other systems (namely a Prometheus
Server) can scrape. When configured, the Prometheus exporter starts an HTTP
server (on whatever <code class="docutils literal notranslate"><span class="pre">endpoint</span></code> you specify, often port 9464 or 8888) and
exposes the collected metrics in Prometheus text format. It collects the
metrics that have arrived via OTLP and holds onto the latest values (or
accumulations) so that on each scrape it can present the current data. In
other words, the Prometheus exporter is <strong>pull-based</strong> – the Collector
passively waits for Prometheus to ask for data
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=,0%3A8889%20namespace%3A%20default">Configuration | OpenTelemetry</a>).
This is in contrast to OTLP or other exporters which are <strong>push-based</strong>
(they initiate a connection and send data out continuously
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=Exporters%20send%20data%20to%20one,one%20or%20more%20data%20sources">Configuration | OpenTelemetry</a>)).</p>
<p><strong>How it works:</strong> Suppose the app sent a gauge metric “cpu_usage” every 5
seconds via OTLP. The Collector’s prometheus exporter will keep updating the
internal state for “cpu_usage”. If a Prometheus server scrapes the Collector
every 15 seconds (hitting the <code class="docutils literal notranslate"><span class="pre">/metrics</span></code> endpoint of the exporter), it will
retrieve the latest values of “cpu_usage” (and all other metrics in that
pipeline) at scrape time. Prometheus then stores those in its time-series
database. The exporter can also apply a <code class="docutils literal notranslate"><span class="pre">namespace</span></code> prefix to metrics (to
avoid collisions) if configured
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=,0%3A8889%20namespace%3A%20default">Configuration | OpenTelemetry</a>).
Because the data is pulled, the <strong>timing</strong> of when metrics are exported is
determined by the Prometheus scrape interval, not by the Collector. The
Collector just continuously updates the metrics in memory between scrapes.</p>
<p>It’s important to note that this is the standard way to integrate
OTel-collected metrics with Prometheus – you make the Collector act like a
Prometheus target. If you <em>instead</em> wanted to push metrics to a remote
system, you could use the <code class="docutils literal notranslate"><span class="pre">prometheusremotewrite</span></code> exporter to send to a
Prometheus remote-write endpoint
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=prometheus%3A%20endpoint%3A%200">Configuration | OpenTelemetry</a>),
but in our config we’re using the scrape model. The difference is summarized
by the Collector docs: <em>“Exporters can be pull or push based”</em>
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=Exporters%20send%20data%20to%20one,one%20or%20more%20data%20sources">Configuration | OpenTelemetry</a>).
The Prometheus exporter is a pull-based exporter (the Collector doesn’t
initiate connection to Prometheus; Prometheus connects to it), whereas the
OTLP exporter (and most other exporters) push data out actively.</p>
</li>
<li><p><strong>Debug Exporter:</strong> The debug exporter (formerly known as the logging
exporter) is used for troubleshooting and development. It simply logs the
telemetry data to the Collector’s console (stdout) or log file. In the
config, it might be listed as <code class="docutils literal notranslate"><span class="pre">exporters:</span> <span class="pre">[debug]</span></code> in all pipelines. This
means every span, metric, and log that passes through will also be printed
out in a human-readable format. The debug exporter can be configured with a
verbosity (e.g. <code class="docutils literal notranslate"><span class="pre">detailed</span></code> to print full data)
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=,debug%3A%20verbosity%3A%20detailed">Configuration | OpenTelemetry</a>).
It’s extremely useful for <strong>verifying that data is actually making it
through the pipeline</strong>. For example, if you’re not seeing data in Jaeger,
enabling the debug exporter allows you to see if the Collector received the
spans and what they look like. In our setup, the debug exporter is likely
enabled for all three pipelines (traces, metrics, logs), so we will see
console output for each signal. This exporter is not meant for production
use (as it can be very verbose and slow), but it’s great for initial
configuration and debugging. (Internally, as of Collector v0.86.0+, the
“logging” exporter was replaced by the “debug” exporter name
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=,debug%3A%20verbosity%3A%20detailed">Configuration | OpenTelemetry</a>),
reflecting its purpose for debugging).</p></li>
</ul>
<p>To summarize exporters: the trace pipeline sends data to two exporters –
OTLP/Jaeger (pushes to Jaeger backend) and debug (logs to console). The metrics
pipeline sends to Prometheus (exposes data for scraping) and debug. The logs
pipeline sends to debug (since in this setup we might not have a separate logs
backend configured). <strong>Multiple exporters can be attached to one pipeline</strong>, and
the Collector will <strong>fan-out</strong> data to all of them. This fan-out is done
efficiently: each exporter gets a copy of the data without interfering with each
other
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=some%20of%20the%20low%20level,paces%20affecting%20each%20other%E2%80%99s%20work">OTel Collector - Blog by Roman Glushko</a>).
For instance, the debug exporter logging a span does not slow down the OTLP
exporter sending spans to Jaeger – the Collector handles them in parallel. This
design means you can safely use a debug exporter alongside real exporters, or
send one stream to two different backends, etc., for flexibility.</p>
</section>
<section id="extensions-health-check-pprof-zpages">
<h3><a class="toc-backref" href="#id23" role="doc-backlink">Extensions – Health Check, Pprof, ZPages</a><a class="headerlink" href="#extensions-health-check-pprof-zpages" title="Link to this heading">#</a></h3>
<p>Extensions are supporting services for the Collector process itself (not for the
telemetry data directly). In our configuration, we have three extensions
enabled: <code class="docutils literal notranslate"><span class="pre">health_check</span></code>, <code class="docutils literal notranslate"><span class="pre">pprof</span></code>, and <code class="docutils literal notranslate"><span class="pre">zpages</span></code>. These help with monitoring and
debugging the Collector in a production environment:</p>
<ul class="simple">
<li><p><strong>Health Check Extension:</strong> This provides a simple HTTP health endpoint (by
default at <code class="docutils literal notranslate"><span class="pre">http://0.0.0.0:13133/</span></code>) that reports the collector’s status
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,is%20ready%20to%20accept%20data">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
Typically it returns HTTP 200 OK if the Collector is running and ready. This
is often used for Kubernetes liveness/readiness probes or other
orchestrators to check if the Collector is up. If the Collector becomes
unhealthy (in future versions, health_check may reflect internal component
health
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=A%20new%20and%20improved%20health,their%20own%20health%20status%20updates">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)),
this endpoint could be used to detect that. In current usage, it mostly
indicates the process is alive and has finished startup. By including
<code class="docutils literal notranslate"><span class="pre">health_check</span></code> in <code class="docutils literal notranslate"><span class="pre">service.extensions</span></code> and configuring it (or using
defaults), you ensure the Collector can be pinged easily. This helps catch
issues where the Collector might crash or hang – your orchestration can
automatically restart it if the health check fails.</p></li>
<li><p><strong>Pprof Extension:</strong> This enables Go’s built-in profiling server (from the
<code class="docutils literal notranslate"><span class="pre">net/http/pprof</span></code> package) on a dedicated port (default is 1777)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,investigate%20issues%20with%20the%20service">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
With pprof enabled, you or developers can connect to the Collector’s pprof
endpoint (e.g. <code class="docutils literal notranslate"><span class="pre">http://collector:1777/debug/pprof/</span></code>) to retrieve performance
profiles: CPU profiles, heap (memory) profiles, goroutine dumps, etc. This
is invaluable when investigating Collector performance issues or memory
leaks in production. For instance, if the Collector’s CPU is spiking, you
can capture a CPU profile via pprof and analyze which functions are
consuming time. The pprof extension essentially makes the Collector
introspectable by Go tooling without needing to modify it. This extension
does not impact the data flowing through the collector; it’s purely for
out-of-band debugging.</p></li>
<li><p><strong>Z-Pages Extension:</strong> Z-Pages provide in-memory monitoring pages for the
Collector’s own telemetry. When enabled (usually on an endpoint like
<code class="docutils literal notranslate"><span class="pre">127.0.0.1:55679</span></code> by default), the Collector will host a small web UI that
shows information about traces and metrics <strong>inside the Collector</strong>
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,in%20troubleshooting%20and%20performance%20optimization">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>).
For example, you can view traces of the Collector’s operations or pipeline
queues, and see aggregated metrics of the Collector’s performance. It’s
somewhat analogous to an “online debug dashboard” for the Collector. This
can be used to see active spans or recently processed traces without needing
an external backend. Essentially, zPages give you a quick way to <strong>verify
what the Collector is receiving and doing</strong> in real time. If telemetry isn’t
making it to your backend, zPages might show if those spans at least reached
the Collector. They also help with internal performance tuning by exposing
metrics. In production, you might not leave zPages open publicly (since it’s
an internal tool), but during development or in restricted environments it’s
very useful.</p></li>
</ul>
<p>All three extensions (<code class="docutils literal notranslate"><span class="pre">health_check</span></code>, <code class="docutils literal notranslate"><span class="pre">pprof</span></code>, <code class="docutils literal notranslate"><span class="pre">zpages</span></code>) are <strong>non-intrusive</strong> –
they don’t alter the telemetry data – but they greatly aid in running the
Collector reliably. Health checks integrate with automation, and pprof/zpages
aid developers in debugging and profiling the service. It’s a best practice to
enable at least health_check and pprof in production Collector deployments
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,is%20ready%20to%20accept%20data">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,investigate%20issues%20with%20the%20service">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>),
so that you have a way to monitor and troubleshoot the Collector itself (after
all, the Collector is part of your observability pipeline, so it too needs to be
observable!).</p>
</section>
</section>
<section id="the-collector-central-telemetry-processor">
<h2><a class="toc-backref" href="#id24" role="doc-backlink">The Collector: Central Telemetry Processor</a><a class="headerlink" href="#the-collector-central-telemetry-processor" title="Link to this heading">#</a></h2>
<p>The OpenTelemetry Collector acts as a gateway between your instrumented
applications and your back-end observability systems. Its design is modular,
separating responsibilities into <strong>receivers</strong>, <strong>processors</strong>, <strong>exporters</strong>,
and <strong>extensions</strong>. Let’s break down each element of the pipeline in
<a class="reference download internal" download="" href="_downloads/bcd84ad6c936b06f226d4b1b1611d809/collector-config.yaml"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">collector-config.yaml</span></code></span></a>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">receivers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">protocols</span><span class="p">:</span>
<span class="w">            </span><span class="nt">grpc</span><span class="p">:</span>
<span class="w">                </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:4317</span>
<span class="w">            </span><span class="nt">http</span><span class="p">:</span>
<span class="w">                </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:4318</span>

<span class="c1"># https://opentelemetry.io/docs/collector/configuration/</span>
<span class="c1"># https://opentelemetry.io/docs/collector/configuration/#processors</span>
<span class="nt">processors</span><span class="p">:</span>
<span class="w">    </span><span class="nt">batch</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># memory_limiter:</span>
<span class="w">    </span><span class="c1">#   check_interval: 5s</span>
<span class="w">    </span><span class="c1">#   limit_mib: 4000</span>
<span class="w">    </span><span class="c1">#   spike_limit_mib: 500</span>
<span class="w">    </span><span class="c1"># resource: # NOTE: if need enable this must add it below in pipelines&#39;s processors</span>
<span class="w">    </span><span class="c1">#   attributes:</span>
<span class="w">    </span><span class="c1">#     - key: service.name</span>
<span class="w">    </span><span class="c1">#       value: v2</span>
<span class="w">    </span><span class="c1">#       action: upsert</span>

<span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">debug</span><span class="p">:</span>
<span class="w">        </span><span class="nt">verbosity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">detailed</span>
<span class="w">        </span><span class="nt">sampling_initial</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">sampling_thereafter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>

<span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:8889</span>
<span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">otel_demo</span>
<span class="w">        </span><span class="nt">send_timestamps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">        </span><span class="nt">resource_to_telemetry_conversion</span><span class="p">:</span>
<span class="w">            </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger:4317</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span>
<span class="w">            </span><span class="nt">insecure</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">extensions</span><span class="p">:</span>
<span class="w">    </span><span class="nt">health_check</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:13133</span>
<span class="w">    </span><span class="nt">pprof</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:1777</span>
<span class="w">    </span><span class="nt">zpages</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:55679</span>

<span class="nt">service</span><span class="p">:</span>
<span class="w">    </span><span class="nt">extensions</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">health_check</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">pprof</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">zpages</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">pipelines</span><span class="p">:</span>
<span class="w">        </span><span class="nt">traces</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">debug</span><span class="p p-Indicator">]</span>

<span class="w">        </span><span class="nt">metrics</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">prometheus</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">debug</span><span class="p p-Indicator">]</span>

<span class="w">        </span><span class="nt">logs</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">debug</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<section id="important-note-on-exporters-understand-this-first">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Important Note on Exporters (Understand this first)</a><a class="headerlink" href="#important-note-on-exporters-understand-this-first" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>The distinction between exporters in the code and in the collector config is
important to understand. The exporters in the code are sending data to the
collector, while the exporters in the collector config are sending data from
the collector to a backend.</p>
</div></blockquote>
</section>
<section id="receivers-receiving-data">
<h3><a class="toc-backref" href="#id26" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">receivers</span></code> - Receiving Data</a><a class="headerlink" href="#receivers-receiving-data" title="Link to this heading">#</a></h3>
<p>This file tells the <strong>OTel Collector</strong> exactly how to behave: where to listen
for data, how to process it, and where to send it.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">receivers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">protocols</span><span class="p">:</span>
<span class="w">            </span><span class="nt">grpc</span><span class="p">:</span>
<span class="w">                </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:4317</span>
<span class="w">            </span><span class="nt">http</span><span class="p">:</span>
<span class="w">                </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:4318</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">receivers</span></code></strong>: This section defines <em>how</em> the collector will <em>receive</em>
telemetry data. It’s the “incoming mail slot” configuration.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">otlp</span></code></strong>: This specifies that the collector should expect data using the
<strong>O</strong>pen<strong>T</strong>e<strong>l</strong>emetry <strong>P</strong>rotocol (OTLP). This is the native OTel
format for traces, metrics, and logs.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">protocols</span></code></strong>: OTLP data can be sent using different underlying network
communication methods (protocols). This configures two common ones:</p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">grpc</span></code></strong>: gRPC is a modern, efficient way for programs to communicate.
The line <code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:4317</span></code> tells the collector to <strong>listen</strong> on
port <strong>4317</strong> (on all network interfaces) for OTLP data sent using the
gRPC protocol. This is the standard default port for OTLP/gRPC.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">http</span></code></strong>: HTTP is the protocol that powers the web. The line
<code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:4318</span></code> tells the collector to also <strong>listen</strong> on port
<strong>4318</strong> (on all network interfaces) for OTLP data sent using the HTTP
protocol. This is the standard default port for OTLP/HTTP.</p></li>
</ul>
</li>
</ul>
<p><strong>In short:</strong> This <code class="docutils literal notranslate"><span class="pre">receivers</span></code> section makes the collector ready to accept OTel
data sent via either gRPC (on port 4317) or HTTP (on port 4318) from any
application that can reach the host machine.</p>
</section>
<section id="processors-modifying-data-in-flight">
<h3><a class="toc-backref" href="#id27" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">processors</span></code> - Modifying Data In-Flight</a><a class="headerlink" href="#processors-modifying-data-in-flight" title="Link to this heading">#</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://opentelemetry.io/docs/collector/configuration/</span>
<span class="nt">processors</span><span class="p">:</span>
<span class="w">    </span><span class="nt">batch</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># memory_limiter: # Maybe enable in future?</span>
<span class="w">    </span><span class="c1">#   # 80% of maximum memory</span>
<span class="w">    </span><span class="c1">#   limit_mib: 2000</span>
<span class="w">    </span><span class="c1">#   # 25% of limit upon exceeding</span>
<span class="w">    </span><span class="c1">#   spike_limit_mib: 500</span>
<span class="w">    </span><span class="c1">#   check_interval: 5s</span>
<span class="w">    </span><span class="c1"># resource: # NOTE: if need enable this must add it below in pipelines&#39;s processors list [...]</span>
<span class="w">    </span><span class="c1">#   attributes:</span>
<span class="w">    </span><span class="c1">#     - key: service.name</span>
<span class="w">    </span><span class="c1">#       value: v2</span>
<span class="w">    </span><span class="c1">#       action: upsert</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>What are Processors?</strong> Once the collector <em>receives</em> data (via the
<code class="docutils literal notranslate"><span class="pre">receivers</span></code>), it can optionally <em>process</em> it before <em>exporting</em> it.
Processors act like stations on an assembly line; they modify the telemetry
data passing through. This could involve:</p>
<ul>
<li><p>Adding extra information (like the environment it’s running in).</p></li>
<li><p>Filtering out unwanted data (e.g., noisy logs).</p></li>
<li><p>Sampling traces (only keeping a percentage of traces to reduce volume).</p></li>
<li><p>Batching data for more efficient sending.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">batch</span></code> Processor:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> This is one of the most common and important processors.
Network communication is often more efficient when sending data in
larger chunks rather than many small pieces. The <code class="docutils literal notranslate"><span class="pre">batch</span></code> processor
collects individual spans (parts of traces), metric data points, or log
entries as they arrive and groups them into batches. It then sends these
batches downstream to the exporters.</p></li>
<li><p><strong>Why Use It:</strong> Sending data piece-by-piece can create a lot of network
overhead (each send requires setup and teardown). Batching reduces this
overhead, leading to better performance for both the collector and the
receiving systems (exporters). It also reduces the number of outgoing
requests.</p></li>
<li><p><strong>Configuration:</strong> In this file, the <code class="docutils literal notranslate"><span class="pre">batch:</span></code> line is present, but it
doesn’t have any specific configuration options underneath it (like
<code class="docutils literal notranslate"><span class="pre">send_batch_size</span></code> or <code class="docutils literal notranslate"><span class="pre">timeout</span></code>). This means it’s using the <em>default</em>
settings for the batch processor provided by the OTel Collector. These
defaults are generally sensible for common use cases.</p></li>
</ul>
</li>
<li><p><strong>Other Processors (Potential Future Use):</strong></p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">memory_limiter</span></code>:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> This processor helps prevent the collector from running
out of memory if data arrives faster than it can be processed and
exported. It monitors the collector’s memory usage.</p></li>
<li><p><strong>How it would work:</strong> If enabled, <code class="docutils literal notranslate"><span class="pre">limit_mib:</span> <span class="pre">2000</span></code> would set a
memory limit (e.g., 2000 MiB). If usage exceeds this, the processor
would start dropping data to relieve pressure. <code class="docutils literal notranslate"><span class="pre">spike_limit_mib</span></code>
adds a buffer for sudden spikes, and <code class="docutils literal notranslate"><span class="pre">check_interval</span></code> defines how
often to check memory usage.</p></li>
<li><p><strong>Relevance:</strong> Useful in high-throughput environments to ensure
collector stability.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">resource</span></code>:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> Telemetry data often includes “resource attributes”
describing the <em>source</em> of the data (e.g., the application name,
version, cloud region). This processor allows you to modify these
attributes.</p></li>
<li><p><strong>How it would work:</strong> If enabled, the example configuration
(<code class="docutils literal notranslate"><span class="pre">attributes:</span> <span class="pre">...</span> <span class="pre">action:</span> <span class="pre">upsert</span></code>) would ensure that every piece of
telemetry data passing through has a resource attribute
<code class="docutils literal notranslate"><span class="pre">service.name</span></code> set to <code class="docutils literal notranslate"><span class="pre">v2</span></code>. If the attribute already exists, it
updates it (<code class="docutils literal notranslate"><span class="pre">upsert</span></code>); if not, it adds it.</p></li>
<li><p><strong>Relevance:</strong> Useful for standardizing identifying information
across all telemetry data originating from a specific collector
instance or environment. The comment
<code class="docutils literal notranslate"><span class="pre">NOTE:</span> <span class="pre">if</span> <span class="pre">need</span> <span class="pre">enable</span> <span class="pre">this</span> <span class="pre">must</span> <span class="pre">add</span> <span class="pre">it</span> <span class="pre">below</span> <span class="pre">in</span> <span class="pre">pipelines's</span> <span class="pre">processors</span> <span class="pre">list</span> <span class="pre">[...]</span></code>
is crucial - just defining a processor here isn’t enough; it must
also be explicitly added to the relevant pipeline(s) in the
<code class="docutils literal notranslate"><span class="pre">service</span></code> section to be activated.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="difference-between-configs-in-sdk-app-level-vs-collector-config">
<h4><a class="toc-backref" href="#id28" role="doc-backlink">Difference between configs in SDK/app level vs collector config</a><a class="headerlink" href="#difference-between-configs-in-sdk-app-level-vs-collector-config" title="Link to this heading">#</a></h4>
<p>Let’s just use <code class="docutils literal notranslate"><span class="pre">batch</span></code> as an example here. In SDK level, there is batch, and in
collector config, there is also batch. There are two separate batch mechanisms
in the OpenTelemetry setup:</p>
<ol class="arabic simple">
<li><p><strong>SDK-level batching</strong>: This happens in your application code via the
OpenTelemetry SDK. The SDK’s batch processor (like <code class="docutils literal notranslate"><span class="pre">BatchSpanProcessor</span></code> for
traces) collects telemetry data within your application process before
sending it to the collector.</p></li>
<li><p><strong>Collector-level batching</strong>: This happens in the OpenTelemetry Collector
after it receives data from your application. The collector’s batch processor
collects telemetry before forwarding it to backend systems.</p></li>
</ol>
<p>The key differences are:</p>
<ul class="simple">
<li><p><strong>SDK batch processor</strong>: Batches data from your application to the collector</p>
<ul>
<li><p>Configured in your application code or environment variables (e.g.,
<code class="docutils literal notranslate"><span class="pre">OTEL_BSP_MAX_EXPORT_BATCH_SIZE</span></code>)</p></li>
<li><p>Controls how efficiently your application sends data to the collector</p></li>
<li><p>Reduces network overhead between your app and the collector</p></li>
</ul>
</li>
<li><p><strong>Collector batch processor</strong>: Batches data from the collector to backend
systems</p>
<ul>
<li><p>Configured in the collector’s config file (<code class="docutils literal notranslate"><span class="pre">collector-config.yaml</span></code>)</p></li>
<li><p>Controls how efficiently the collector sends data to backends like
Jaeger/Prometheus</p></li>
<li><p>Reduces network overhead between the collector and backends</p></li>
</ul>
</li>
</ul>
<p>Both batch processors work independently and can have different configurations.
The collector’s batch processor can override or supplement the SDK’s batching
behavior, providing an additional layer of optimization for forwarding data to
backends.</p>
</section>
</section>
<section id="exporters-sending-data-out">
<h3><a class="toc-backref" href="#id29" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">exporters</span></code> - Sending Data Out</a><a class="headerlink" href="#exporters-sending-data-out" title="Link to this heading">#</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">debug</span><span class="p">:</span>
<span class="w">        </span><span class="nt">verbosity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">detailed</span>
<span class="w">        </span><span class="nt">sampling_initial</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">sampling_thereafter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>

<span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:8889</span>
<span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">otel_demo</span>
<span class="w">        </span><span class="nt">send_timestamps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">        </span><span class="nt">resource_to_telemetry_conversion</span><span class="p">:</span>
<span class="w">            </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger:4317</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span>
<span class="w">            </span><span class="nt">insecure</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>What are Exporters?</strong> After data has been received and potentially
processed, exporters are responsible for <em>sending</em> it to its final
destination(s). These destinations are typically backend systems designed
for storing, visualizing, and analyzing telemetry data.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">debug</span></code> Exporter:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> This is primarily used for debugging and development.
Instead of sending data to a real backend, it prints the telemetry data
it receives directly to the collector’s own console output (standard out
or standard error).</p></li>
<li><p><strong>Configuration:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">verbosity:</span> <span class="pre">detailed</span></code>: This tells the exporter to print the <em>full</em>
contents of the traces, metrics, and logs it receives. Other options
might be <code class="docutils literal notranslate"><span class="pre">basic</span></code> or <code class="docutils literal notranslate"><span class="pre">normal</span></code> for less output. <code class="docutils literal notranslate"><span class="pre">detailed</span></code> is very
useful when troubleshooting to see exactly what data the collector
is handling.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sampling_initial:</span> <span class="pre">5</span></code>: Print the first 5 batches of data received in
detail.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sampling_thereafter:</span> <span class="pre">200</span></code>: After the initial samples, only print
one out of every 200 batches received. This prevents the console
from being flooded with excessive output in long-running or
high-traffic scenarios while still providing occasional glimpses of
the data flow.</p></li>
</ul>
</li>
<li><p><strong>Relevance:</strong> Excellent for verifying that the collector is receiving
data as expected and understanding the structure of the data <em>before</em>
configuring complex backends.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">prometheus</span></code> Exporter:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> Prometheus is a very popular open-source system for
collecting and querying time-series <em>metrics</em>. It works on a “pull”
model: Prometheus itself periodically connects to configured targets
(like this exporter) and “scrapes” (requests) the latest metric values.
This exporter makes the OTel Collector’s received metrics available for
a Prometheus server to scrape.</p></li>
<li><p><strong>Configuration:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:8889</span></code>: This tells the <code class="docutils literal notranslate"><span class="pre">prometheus</span></code> exporter
<em>within the collector</em> to start its own tiny web server,
<strong>listening</strong> on port <strong>8889</strong> (on all network interfaces of the
collector’s host/container). When a Prometheus server connects to
this address (<code class="docutils literal notranslate"><span class="pre">&lt;collector_ip&gt;:8889/metrics</span></code>), this exporter will
respond with the current metric data in the format Prometheus
understands. It’s the reverse of a receiver; here the collector
<em>provides</em> an endpoint for something else (Prometheus) to connect
<em>to</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">namespace:</span> <span class="pre">otel_demo</span></code>: Prefixes all metric names exposed by this
exporter with <code class="docutils literal notranslate"><span class="pre">otel_demo_</span></code>. This helps avoid naming collisions if
Prometheus is scraping metrics from multiple sources.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">send_timestamps:</span> <span class="pre">true</span></code>: Includes the original timestamp of the
metric data point when exposing it to Prometheus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resource_to_telemetry_conversion:</span> <span class="pre">enabled:</span> <span class="pre">true</span></code>: Attempts to
convert OTel resource attributes (like <code class="docutils literal notranslate"><span class="pre">service.name</span></code>) into
Prometheus labels attached to the metrics, allowing for richer
filtering and aggregation in Prometheus.</p></li>
</ul>
</li>
<li><p><strong>Relevance:</strong> This is the standard way to feed metrics collected by
OTel into a Prometheus monitoring stack.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">otlp</span></code> Exporter:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> This exporter sends telemetry data <em>out</em> using the standard
OTLP protocol (just like the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> <em>receiver</em> accepted data <em>in</em> using
OTLP). It’s designed to send data to another OTel Collector or any
backend system that understands OTLP (like Jaeger, Tempo, Grafana Cloud,
etc.).</p></li>
<li><p><strong>Configuration:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">jaeger:4317</span></code>: This specifies the destination address for
the OTLP data. Unlike the <code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code> used for <em>listening</em>, this is a
specific target. <code class="docutils literal notranslate"><span class="pre">jaeger:4317</span></code> likely means: “Send the data to a
host named <code class="docutils literal notranslate"><span class="pre">jaeger</span></code> on port <code class="docutils literal notranslate"><span class="pre">4317</span></code>.” In a containerized environment
(like Docker Compose or Kubernetes, which are hinted at by the
filenames in your workspace), <code class="docutils literal notranslate"><span class="pre">jaeger</span></code> is often the service name of
another container running the Jaeger tracing system. That Jaeger
instance would be <em>listening</em> on its port 4317 (likely configured to
receive OTLP/gRPC). Push based: Data is pushed over the OTLP
protocol to the Jaeger service running on port 4317 (as defined in
your exporter configuration).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tls:</span> <span class="pre">insecure:</span> <span class="pre">true</span></code>: TLS (Transport Layer Security) is the
standard for encrypting network communication (like HTTPS for
websites). This setting <code class="docutils literal notranslate"><span class="pre">insecure:</span> <span class="pre">true</span></code> explicitly <em>disables</em> TLS
encryption for the connection <em>from the collector to Jaeger</em>. This
is common in development or internal networks where encryption might
be deemed unnecessary overhead, but <strong>it should generally NOT be
used in production environments handling sensitive data over
untrusted networks.</strong> In production, you would configure TLS
properly with certificates for secure communication.</p></li>
</ul>
</li>
<li><p><strong>Relevance:</strong> This is the primary way to forward OTel data to
OTel-native backends or other collectors in a pipeline. Here, it’s
specifically configured to send data (likely traces, as Jaeger is a
tracing system) to a Jaeger backend.</p></li>
</ul>
</li>
</ul>
<section id="docker-dns-resolution-and-why-hostname-is-jaeger">
<h4><a class="toc-backref" href="#id30" role="doc-backlink">Docker DNS Resolution And Why Hostname is Jaeger?</a><a class="headerlink" href="#docker-dns-resolution-and-why-hostname-is-jaeger" title="Link to this heading">#</a></h4>
<p>In Docker Compose environments, the service name automatically becomes the
hostname that other services can use to reach that container. This is a key
feature of Docker’s built-in DNS resolution for custom networks.</p>
<p>When you have this in your collector configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otlp</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">jaeger:4317</span>
<span class="w">        </span><span class="nt">tls</span><span class="p">:</span>
<span class="w">            </span><span class="nt">insecure</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>And these services defined in your docker-compose file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">services</span><span class="p">:</span>
<span class="w">    </span><span class="nt">otel-collector</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># configuration...</span>
<span class="w">        </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">otel-demo</span>

<span class="w">    </span><span class="nt">jaeger</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># configuration...</span>
<span class="w">        </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">otel-demo</span>
</pre></div>
</div>
<p>Docker automatically creates DNS entries for each service, allowing them to
communicate using the service name as a hostname. So <code class="docutils literal notranslate"><span class="pre">jaeger:4317</span></code> in the
collector config means “connect to port 4317 on the container running the jaeger
service.”</p>
<p>This works because:</p>
<ol class="arabic simple">
<li><p>Both containers are on the same Docker network (<code class="docutils literal notranslate"><span class="pre">otel-demo</span></code>)</p></li>
<li><p>Docker provides automatic DNS resolution within custom networks</p></li>
<li><p>Service names in docker-compose.yml become hostnames in that network</p></li>
</ol>
<p>This is much more convenient than using IP addresses because:</p>
<ul class="simple">
<li><p>Container IPs can change when containers restart</p></li>
<li><p>You don’t need to manually discover or configure IP addresses</p></li>
<li><p>It works the same in development and production environments</p></li>
</ul>
<p>If you were to deploy this in Kubernetes instead of Docker Compose, the same
concept applies - service names would be used as hostnames to communicate
between pods in the same namespace.</p>
</section>
<section id="debug-exporter">
<h4><a class="toc-backref" href="#id31" role="doc-backlink">Debug Exporter</a><a class="headerlink" href="#debug-exporter" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> exporter in the collector configuration is essentially the
equivalent of adding console logging in your application code. They serve
similar purposes but operate at different points in the telemetry pipeline.</p>
<section id="debug-exporter-in-collector-config">
<h5><a class="toc-backref" href="#id32" role="doc-backlink">Debug Exporter in Collector Config</a><a class="headerlink" href="#debug-exporter-in-collector-config" title="Link to this heading">#</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> exporter in your collector configuration (which you can see in the
config as <code class="docutils literal notranslate"><span class="pre">exporters:</span> <span class="pre">[debug]</span></code> for each pipeline) does the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">exporters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">debug</span><span class="p">:</span>
<span class="w">        </span><span class="nt">verbosity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">detailed</span>
<span class="w">        </span><span class="nt">sampling_initial</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">sampling_thereafter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><strong>Purpose</strong>: Prints telemetry data to the collector’s console/stdout for
debugging and verification</p></li>
<li><p><strong>Configuration</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">verbosity:</span> <span class="pre">detailed</span></code> - Shows the full content of spans, metrics, and logs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sampling_initial:</span> <span class="pre">5</span></code> - Prints the first 5 batches of data in detail</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sampling_thereafter:</span> <span class="pre">200</span></code> - After that, only prints 1 out of every 200
batches</p></li>
</ul>
</li>
<li><p><strong>Usage examples</strong>:</p>
<ul class="simple">
<li><p>Verifying that data is reaching the collector</p></li>
<li><p>Troubleshooting why data might not be appearing in backends like Jaeger</p></li>
<li><p>Inspecting the format and content of telemetry as it passes through the
collector</p></li>
</ul>
</li>
<li><p><strong>Where output appears</strong>: In the collector’s logs/console (e.g., if running
in Docker, you’d see it with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">logs</span> <span class="pre">otel-collector</span></code>)</p></li>
</ol>
</section>
<section id="console-logging-in-application-code">
<h5><a class="toc-backref" href="#id33" role="doc-backlink">Console Logging in Application Code</a><a class="headerlink" href="#console-logging-in-application-code" title="Link to this heading">#</a></h5>
<p>When you add console logging in your application’s instrumentation code:</p>
<ol class="arabic">
<li><p><strong>Purpose</strong>: Prints telemetry data to your application’s stdout for debugging</p></li>
<li><p><strong>Implementation</strong>: Might use a console exporter like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Python example</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opentelemetry.sdk.trace.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConsoleSpanExporter</span>

<span class="n">console_exporter</span> <span class="o">=</span> <span class="n">ConsoleSpanExporter</span><span class="p">()</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">SimpleSpanProcessor</span><span class="p">(</span><span class="n">console_exporter</span><span class="p">)</span>
<span class="n">tracer_provider</span><span class="o">.</span><span class="n">add_span_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Usage examples</strong>:</p>
<ul class="simple">
<li><p>Debugging during development</p></li>
<li><p>Verifying spans are being created correctly</p></li>
<li><p>Checking if context propagation is working</p></li>
</ul>
</li>
<li><p><strong>Where output appears</strong>: In your application’s console output</p></li>
</ol>
</section>
<section id="key-differences">
<h5><a class="toc-backref" href="#id34" role="doc-backlink">Key Differences</a><a class="headerlink" href="#key-differences" title="Link to this heading">#</a></h5>
<ol class="arabic simple">
<li><p><strong>Location in pipeline</strong>:</p>
<ul class="simple">
<li><p>App console logging: Shows data as it’s generated in your application</p></li>
<li><p>Collector debug exporter: Shows data after it’s been received and
processed by the collector</p></li>
</ul>
</li>
<li><p><strong>Detail and format</strong>:</p>
<ul class="simple">
<li><p>App logging: Format depends on the SDK’s implementation</p></li>
<li><p>Collector debug: Consistent format with configurable verbosity</p></li>
</ul>
</li>
<li><p><strong>Sampling control</strong>:</p>
<ul class="simple">
<li><p>App logging: Depends on how you configure it (often shows all spans)</p></li>
<li><p>Collector debug: Has built-in sampling to prevent overwhelming logs</p></li>
</ul>
</li>
</ol>
</section>
<section id="when-to-use-each">
<h5><a class="toc-backref" href="#id35" role="doc-backlink">When to Use Each</a><a class="headerlink" href="#when-to-use-each" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Application console logging</strong>: During initial development to confirm your
instrumentation is working correctly</p></li>
<li><p><strong>Collector debug exporter</strong>: When testing the full pipeline to verify data
is flowing through the collector correctly</p></li>
</ul>
<p>In practice, the collector’s debug exporter is more powerful because:</p>
<ol class="arabic simple">
<li><p>It can show all three signal types (traces, metrics, logs)</p></li>
<li><p>It shows data after any transformations by the collector’s processors</p></li>
<li><p>It has built-in sampling to manage log volume in busy systems</p></li>
<li><p>It can confirm the data successfully made it to the collector</p></li>
</ol>
<p>Both serve the purpose of logging telemetry for debugging, just at different
stages of the flow.</p>
</section>
</section>
</section>
<section id="extensions-adding-auxiliary-capabilities">
<h3><a class="toc-backref" href="#id36" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">extensions</span></code> - Adding Auxiliary Capabilities</a><a class="headerlink" href="#extensions-adding-auxiliary-capabilities" title="Link to this heading">#</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">extensions</span><span class="p">:</span>
<span class="w">    </span><span class="nt">health_check</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:13133</span>
<span class="w">    </span><span class="nt">pprof</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:1777</span>
<span class="w">    </span><span class="nt">zpages</span><span class="p">:</span>
<span class="w">        </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0.0.0:55679</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>What are Extensions?</strong> Extensions add functionality to the collector that
isn’t directly part of the data processing pipeline (receive -&gt; process -&gt;
export). They often provide ways to monitor the collector itself or expose
diagnostic interfaces.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">health_check</span></code> Extension:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> Provides a simple way to check if the collector process is
running and healthy. Monitoring systems or orchestrators (like
Kubernetes) can query this endpoint to determine if the collector is
operational.</p></li>
<li><p><strong>Configuration:</strong> <code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:13133</span></code> tells the health check
extension to <strong>listen</strong> on port <strong>13133</strong> (on all interfaces). If you
make an HTTP request to <code class="docutils literal notranslate"><span class="pre">http://&lt;collector_ip&gt;:13133</span></code>, it will respond
with a status code indicating health (e.g., <code class="docutils literal notranslate"><span class="pre">200</span> <span class="pre">OK</span></code> if healthy).</p></li>
<li><p><strong>Relevance:</strong> Essential for automated monitoring and ensuring the
collector is running correctly.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">pprof</span></code> Extension:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> The OTel Collector is written in the Go programming
language. <code class="docutils literal notranslate"><span class="pre">pprof</span></code> is Go’s standard tool for profiling performance (CPU
usage, memory allocation). Enabling this extension allows developers or
operators to connect to the collector and gather detailed performance
profiles.</p></li>
<li><p><strong>Configuration:</strong> <code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:1777</span></code> tells the pprof extension to
<strong>listen</strong> on port <strong>1777</strong> (on all interfaces), exposing the profiling
data endpoints. Tools can then connect to this port to request profiles.</p></li>
<li><p><strong>Relevance:</strong> Useful for diagnosing performance bottlenecks <em>within the
collector itself</em> if it seems to be consuming too many resources.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">zpages</span></code> Extension:</strong></p>
<ul>
<li><p><strong>Purpose:</strong> zPages are web-based diagnostic pages, originally developed
at Google, that provide visibility into the internal state of running
processes. For the collector, this can show information about configured
pipelines, connected clients, internal metrics, etc.</p></li>
<li><p><strong>Configuration:</strong> <code class="docutils literal notranslate"><span class="pre">endpoint:</span> <span class="pre">0.0.0.0:55679</span></code> tells the zPages extension
to <strong>listen</strong> on port <strong>55679</strong> (on all interfaces). You can then open
<code class="docutils literal notranslate"><span class="pre">http://&lt;collector_ip&gt;:55679</span></code> in a web browser to view the diagnostic
pages.</p></li>
<li><p><strong>Relevance:</strong> Provides a human-friendly way to inspect the collector’s
internal state for debugging and understanding its behavior without
needing specialized tools (like pprof).</p></li>
</ul>
</li>
</ul>
</section>
<section id="service-tying-everything-together">
<h3><a class="toc-backref" href="#id37" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">service</span></code> - Tying Everything Together</a><a class="headerlink" href="#service-tying-everything-together" title="Link to this heading">#</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">service</span><span class="p">:</span>
<span class="w">    </span><span class="nt">extensions</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">health_check</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">pprof</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">zpages</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">pipelines</span><span class="p">:</span>
<span class="w">        </span><span class="nt">traces</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">debug</span><span class="p p-Indicator">]</span>

<span class="w">        </span><span class="nt">metrics</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">prometheus</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">debug</span><span class="p p-Indicator">]</span>

<span class="w">        </span><span class="nt">logs</span><span class="p">:</span>
<span class="w">            </span><span class="nt">receivers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">otlp</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">processors</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">batch</span><span class="p p-Indicator">]</span>
<span class="w">            </span><span class="nt">exporters</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">debug</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>What is the <code class="docutils literal notranslate"><span class="pre">service</span></code> Section?</strong> This is the conductor of the orchestra.
It defines <em>which</em> components (receivers, processors, exporters, extensions)
are actually <em>active</em> and <em>how they are connected</em>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">extensions:</span> <span class="pre">[health_check,</span> <span class="pre">pprof,</span> <span class="pre">zpages]</span></code>:</strong> This line explicitly
<em>enables</em> the extensions that were defined earlier in the <code class="docutils literal notranslate"><span class="pre">extensions:</span></code>
block. If an extension was defined but not listed here, it wouldn’t be
started.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">pipelines</span></code>:</strong> This is the core of the service definition. A pipeline
defines the path data takes from input to output for a specific <em>type</em> of
telemetry signal (traces, metrics, or logs). You can define multiple
pipelines if needed, but here there’s one for each signal type.</p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">traces</span></code> Pipeline:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">receivers:</span> <span class="pre">[otlp]</span></code>: This pipeline gets its input data from the
receiver named <code class="docutils literal notranslate"><span class="pre">otlp</span></code> (which we defined earlier to listen on ports
4317 and 4318).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">processors:</span> <span class="pre">[batch]</span></code>: The trace data received will pass through the
<code class="docutils literal notranslate"><span class="pre">batch</span></code> processor. If we had uncommented the <code class="docutils literal notranslate"><span class="pre">resource</span></code> processor
earlier, we would add it here like <code class="docutils literal notranslate"><span class="pre">processors:</span> <span class="pre">[batch,</span> <span class="pre">resource]</span></code>
(order matters!).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exporters:</span> <span class="pre">[otlp,</span> <span class="pre">debug]</span></code>: After processing, the trace data is sent
to <em>two</em> destinations: the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> exporter (which sends it to
<code class="docutils literal notranslate"><span class="pre">jaeger:4317</span></code>) and the <code class="docutils literal notranslate"><span class="pre">debug</span></code> exporter (which prints it to the
console).</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">metrics</span></code> Pipeline:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">receivers:</span> <span class="pre">[otlp]</span></code>: Also receives data from the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> receiver.
Note that the same receiver can handle multiple signal types if
configured to do so (OTLP natively supports traces, metrics, and
logs).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">processors:</span> <span class="pre">[batch]</span></code>: Metric data also goes through the <code class="docutils literal notranslate"><span class="pre">batch</span></code>
processor.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exporters:</span> <span class="pre">[prometheus,</span> <span class="pre">debug]</span></code>: Metric data is sent to the
<code class="docutils literal notranslate"><span class="pre">prometheus</span></code> exporter (making it available on port 8889 for
scraping) and the <code class="docutils literal notranslate"><span class="pre">debug</span></code> exporter.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">logs</span></code> Pipeline:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">receivers:</span> <span class="pre">[otlp]</span></code>: Receives logs from the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> receiver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">processors:</span> <span class="pre">[batch]</span></code>: Log data also goes through the <code class="docutils literal notranslate"><span class="pre">batch</span></code>
processor.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exporters:</span> <span class="pre">[debug]</span></code>: Log data is <em>only</em> sent to the <code class="docutils literal notranslate"><span class="pre">debug</span></code>
exporter in this configuration. It’s not being sent to Prometheus
(which is primarily for metrics) or the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> exporter (though it
could be, if Jaeger or another backend was configured to receive
OTLP logs).</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="summary">
<h3><a class="toc-backref" href="#id38" role="doc-backlink">Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<p>This configuration file sets up an OTel Collector that:</p>
<ol class="arabic simple">
<li><p><strong>Listens</strong> for OTLP data (traces, metrics, logs) via both gRPC (port 4317)
and HTTP (port 4318) using the <code class="docutils literal notranslate"><span class="pre">otlp</span></code> receiver.</p></li>
<li><p><strong>Processes</strong> all received data by batching it for efficiency using the
<code class="docutils literal notranslate"><span class="pre">batch</span></code> processor.</p></li>
<li><p><strong>Exports</strong> the data based on its type:</p>
<ul class="simple">
<li><p><strong>Traces:</strong> Sent via OTLP to a Jaeger instance (at <code class="docutils literal notranslate"><span class="pre">jaeger:4317</span></code>) AND
printed to the console (<code class="docutils literal notranslate"><span class="pre">debug</span></code>).</p></li>
<li><p><strong>Metrics:</strong> Made available for a Prometheus server to scrape (on
port 8889) AND printed to the console (<code class="docutils literal notranslate"><span class="pre">debug</span></code>).</p></li>
<li><p><strong>Logs:</strong> Only printed to the console (<code class="docutils literal notranslate"><span class="pre">debug</span></code>).</p></li>
</ul>
</li>
<li><p><strong>Enables</strong> supporting services (<code class="docutils literal notranslate"><span class="pre">extensions</span></code>): a health check (port 13133),
performance profiling (port 1777), and diagnostic web pages (port 55679).</p></li>
</ol>
<p>This setup is typical for a development or testing environment where you want to
send traces to Jaeger, metrics to Prometheus, and see all data easily via the
debug output, while also having tools to check the collector’s health and
performance.</p>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id39" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>OpenTelemetry Collector as a vendor-agnostic pipeline
(<a class="reference external" href="https://uptrace.dev/opentelemetry/collector#:~:text=OpenTelemetry%20Collector%20serves%20as%20a,such%20as%20Uptrace%20or%20Jaeger">Getting Started with the OpenTelemetry Collector | Uptrace</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=Direct%20telemetry%20reporting%20or%20using,experiment%20with%20multiple%20backends%20simultaneously">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)</p></li>
<li><p>OpenTelemetry Protocol (OTLP) overview and characteristics
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=OTLP%20is%20a%20telemetry%20data,forwarders%2C%20and%20various%20observability%20backends">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/otlp/#:~:text=,gRPC%20may%20not%20be%20ideal">A Deep Dive into the OpenTelemetry Protocol (OTLP) | Better Stack Community</a>)</p></li>
<li><p>Collector processors (batch, memory_limiter, resource) and their benefits
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=4,delivery">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
(<a class="reference external" href="https://www.romaglushko.com/blog/opentelemetry-collector/#:~:text=The%20memory%20limiter%20checks%20service,any%20of%20the%20defined%20limits">OTel Collector - Blog by Roman Glushko</a>)
(<a class="reference external" href="https://docs.splunk.com/observability/gdi/opentelemetry/components/resource-processor.html#:~:text=The%20resource%20processor%20is%20an,with%20pipelines%20for%20more%20information">Resource processor — Splunk Observability Cloud documentation</a>)</p></li>
<li><p>Collector exporters and modes (push vs pull, OTLP vs Prometheus)
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=Exporters%20send%20data%20to%20one,one%20or%20more%20data%20sources">Configuration | OpenTelemetry</a>)
(<a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/#:~:text=,0%3A8889%20namespace%3A%20default">Configuration | OpenTelemetry</a>)</p></li>
<li><p>Collector extensions for health, profiling, and debugging
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,is%20ready%20to%20accept%20data">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=The%20,in%20troubleshooting%20and%20performance%20optimization">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)</p></li>
<li><p>Common OTLP configuration pitfalls (4317 vs 4318, protocol setting)
(<a class="reference external" href="https://stackoverflow.com/questions/72099467/error-io-opentelemetry-exporter-internal-grpc-okhttpgrpcexporter#:~:text=opentelemetry,Dotel.exporter.otlp.protocol%3Dhttp%2Fprotobuf">open telemetry - ERROR io.opentelemetry.exporter.internal.grpc.OkHttpGrpcExporter - Stack Overflow</a>)</p></li>
<li><p>Jaeger OTLP exporter configuration example
(<a class="reference external" href="https://betterstack.com/community/guides/observability/opentelemetry-collector/#:~:text=exporters%3A%20otlp%2Fjaeger%3A%20endpoint%3A%20jaeger%3A4317%20tls%3A,insecure%3A%20true">A Beginner’s Guide to the OpenTelemetry Collector | Better Stack Community</a>)
and native support in Jaeger</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="flow.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Flow</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-understanding-complex-systems">The Problem: Understanding Complex Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-observability-with-opentelemetry">The Solution: Observability with OpenTelemetry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#managing-telemetry-data-with-the-otel-collector">Managing Telemetry Data with the OTel Collector</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-networking-concepts-for-services-and-containers">Basic Networking Concepts for Services and Containers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#networking-101">Networking 101</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#networking-in-containers-docker-docker-compose">Networking in Containers (Docker / Docker Compose)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-telemetry-flow">Anatomy of Telemetry Flow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generation-phase-application-level-telemetry-instantiation">1. Generation Phase: Application-Level Telemetry Instantiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transmission-phase-otlp-network-protocol">2. Transmission Phase: OTLP Network Protocol</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reception-phase-collector-data-ingestion">3. Reception Phase: Collector Data Ingestion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-phase-collector-pipeline-execution">4. Processing Phase: Collector Pipeline Execution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exportation-phase-data-egress-to-backends">5. Exportation Phase: Data Egress to Backends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storage-analysis-phase-backend-systems">6. Storage &amp; Analysis Phase: Backend Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#systemic-considerations">7. Systemic Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anatomy-of-telemetry-flow-deep-research-from-openai-we-keep-it">Anatomy of Telemetry Flow (Deep Research From OpenAI, We Keep It)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instrumentation-in-the-application-opentelemetry-sdk">Instrumentation in the Application (OpenTelemetry SDK)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#telemetry-export-via-otlp-opentelemetry-protocol">Telemetry Export via OTLP (OpenTelemetry Protocol)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-opentelemetry-collectors-role-and-components">The OpenTelemetry Collector’s Role and Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receivers-otlp-receiver-grpc-http">Receivers – OTLP Receiver (gRPC/HTTP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processors-batch-memory-limiter-resource">Processors – Batch, Memory Limiter, Resource</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exporters-otlp-to-jaeger-prometheus-debug">Exporters – OTLP (to Jaeger), Prometheus, Debug</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-health-check-pprof-zpages">Extensions – Health Check, Pprof, ZPages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-collector-central-telemetry-processor">The Collector: Central Telemetry Processor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-note-on-exporters-understand-this-first">Important Note on Exporters (Understand this first)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receivers-receiving-data"><code class="docutils literal notranslate"><span class="pre">receivers</span></code> - Receiving Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processors-modifying-data-in-flight"><code class="docutils literal notranslate"><span class="pre">processors</span></code> - Modifying Data In-Flight</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-configs-in-sdk-app-level-vs-collector-config">Difference between configs in SDK/app level vs collector config</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exporters-sending-data-out"><code class="docutils literal notranslate"><span class="pre">exporters</span></code> - Sending Data Out</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#docker-dns-resolution-and-why-hostname-is-jaeger">Docker DNS Resolution And Why Hostname is Jaeger?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-exporter">Debug Exporter</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-exporter-in-collector-config">Debug Exporter in Collector Config</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#console-logging-in-application-code">Console Logging in Application Code</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-each">When to Use Each</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-adding-auxiliary-capabilities"><code class="docutils literal notranslate"><span class="pre">extensions</span></code> - Adding Auxiliary Capabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#service-tying-everything-together"><code class="docutils literal notranslate"><span class="pre">service</span></code> - Tying Everything Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hongnan Gao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>